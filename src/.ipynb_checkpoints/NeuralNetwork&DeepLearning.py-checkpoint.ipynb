{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using python 2.7\n",
    "# test on mnist dataset\n",
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 9116 / 10000\n",
      "Epoch 1: 9154 / 10000\n",
      "Epoch 2: 9294 / 10000\n",
      "Epoch 3: 9357 / 10000\n",
      "Epoch 4: 9413 / 10000\n",
      "Epoch 5: 9381 / 10000\n",
      "Epoch 6: 9389 / 10000\n",
      "Epoch 7: 9426 / 10000\n",
      "Epoch 8: 9424 / 10000\n",
      "Epoch 9: 9411 / 10000\n",
      "Epoch 10: 9435 / 10000\n",
      "Epoch 11: 9432 / 10000\n",
      "Epoch 12: 9468 / 10000\n",
      "Epoch 13: 9425 / 10000\n",
      "Epoch 14: 9433 / 10000\n",
      "Epoch 15: 9454 / 10000\n",
      "Epoch 16: 9471 / 10000\n",
      "Epoch 17: 9445 / 10000\n",
      "Epoch 18: 9466 / 10000\n",
      "Epoch 19: 9483 / 10000\n",
      "Epoch 20: 9465 / 10000\n",
      "Epoch 21: 9483 / 10000\n",
      "Epoch 22: 9481 / 10000\n",
      "Epoch 23: 9477 / 10000\n",
      "Epoch 24: 9476 / 10000\n",
      "Epoch 25: 9462 / 10000\n",
      "Epoch 26: 9494 / 10000\n",
      "Epoch 27: 9488 / 10000\n",
      "Epoch 28: 9474 / 10000\n",
      "Epoch 29: 9483 / 10000\n"
     ]
    }
   ],
   "source": [
    "import network\n",
    "net = network.Network([784, 30, 10])\n",
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 7685 / 10000\n",
      "Epoch 1: 7762 / 10000\n",
      "Epoch 2: 7846 / 10000\n",
      "Epoch 3: 8375 / 10000\n",
      "Epoch 4: 8623 / 10000\n",
      "Epoch 5: 8678 / 10000\n",
      "Epoch 6: 8679 / 10000\n",
      "Epoch 7: 8703 / 10000\n",
      "Epoch 8: 8700 / 10000\n",
      "Epoch 9: 9496 / 10000\n",
      "Epoch 10: 9586 / 10000\n",
      "Epoch 11: 9576 / 10000\n",
      "Epoch 12: 9604 / 10000\n",
      "Epoch 13: 9615 / 10000\n",
      "Epoch 14: 9627 / 10000\n",
      "Epoch 15: 9621 / 10000\n",
      "Epoch 16: 9639 / 10000\n",
      "Epoch 17: 9620 / 10000\n",
      "Epoch 18: 9656 / 10000\n",
      "Epoch 19: 9651 / 10000\n",
      "Epoch 20: 9639 / 10000\n",
      "Epoch 21: 9644 / 10000\n",
      "Epoch 22: 9647 / 10000\n",
      "Epoch 23: 9647 / 10000\n",
      "Epoch 24: 9650 / 10000\n",
      "Epoch 25: 9645 / 10000\n",
      "Epoch 26: 9646 / 10000\n",
      "Epoch 27: 9645 / 10000\n",
      "Epoch 28: 9664 / 10000\n",
      "Epoch 29: 9667 / 10000\n"
     ]
    }
   ],
   "source": [
    "#rerun the test and changing the number of hidden neurons to  100\n",
    "net = network.Network([784, 100, 10])\n",
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 969 / 10000\n",
      "Epoch 1: 983 / 10000\n",
      "Epoch 2: 987 / 10000\n",
      "Epoch 3: 867 / 10000\n",
      "Epoch 4: 783 / 10000\n",
      "Epoch 5: 804 / 10000\n",
      "Epoch 6: 819 / 10000\n",
      "Epoch 7: 819 / 10000\n",
      "Epoch 8: 825 / 10000\n",
      "Epoch 9: 834 / 10000\n",
      "Epoch 10: 844 / 10000\n",
      "Epoch 11: 853 / 10000\n",
      "Epoch 12: 861 / 10000\n",
      "Epoch 13: 868 / 10000\n",
      "Epoch 14: 871 / 10000\n",
      "Epoch 15: 876 / 10000\n",
      "Epoch 16: 882 / 10000\n",
      "Epoch 17: 887 / 10000\n",
      "Epoch 18: 895 / 10000\n",
      "Epoch 19: 906 / 10000\n",
      "Epoch 20: 911 / 10000\n",
      "Epoch 21: 920 / 10000\n",
      "Epoch 22: 929 / 10000\n",
      "Epoch 23: 935 / 10000\n",
      "Epoch 24: 943 / 10000\n",
      "Epoch 25: 948 / 10000\n",
      "Epoch 26: 953 / 10000\n",
      "Epoch 27: 957 / 10000\n",
      "Epoch 28: 962 / 10000\n",
      "Epoch 29: 972 / 10000\n"
     ]
    }
   ],
   "source": [
    "#test learning rate = 0.001\n",
    "net = network.Network([784, 100, 10])\n",
    "net.SGD(training_data, 30, 10, 0.001, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1353 / 10000\n",
      "Epoch 1: 1353 / 10000\n",
      "Epoch 2: 1354 / 10000\n",
      "Epoch 3: 1353 / 10000\n",
      "Epoch 4: 1354 / 10000\n",
      "Epoch 5: 1354 / 10000\n",
      "Epoch 6: 1354 / 10000\n",
      "Epoch 7: 1354 / 10000\n",
      "Epoch 8: 1354 / 10000\n",
      "Epoch 9: 1353 / 10000\n",
      "Epoch 10: 1354 / 10000\n",
      "Epoch 11: 1354 / 10000\n",
      "Epoch 12: 1354 / 10000\n",
      "Epoch 13: 1355 / 10000\n",
      "Epoch 14: 1355 / 10000\n",
      "Epoch 15: 1355 / 10000\n",
      "Epoch 16: 1355 / 10000\n",
      "Epoch 17: 1355 / 10000\n",
      "Epoch 18: 1356 / 10000\n",
      "Epoch 19: 1356 / 10000\n",
      "Epoch 20: 1356 / 10000\n",
      "Epoch 21: 1356 / 10000\n",
      "Epoch 22: 1357 / 10000\n",
      "Epoch 23: 1357 / 10000\n",
      "Epoch 24: 1356 / 10000\n",
      "Epoch 25: 1357 / 10000\n",
      "Epoch 26: 1358 / 10000\n",
      "Epoch 27: 1358 / 10000\n",
      "Epoch 28: 1357 / 10000\n",
      "Epoch 29: 1357 / 10000\n"
     ]
    }
   ],
   "source": [
    "#test learning rate = 100.0\n",
    "net = network.Network([784, 30, 10])\n",
    "net.SGD(training_data, 30, 10, 100.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing cross entropy\n",
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on evaluation data: 9140 / 10000\n",
      "\n",
      "Epoch 1 training complete\n",
      "Accuracy on evaluation data: 9281 / 10000\n",
      "\n",
      "Epoch 2 training complete\n",
      "Accuracy on evaluation data: 9338 / 10000\n",
      "\n",
      "Epoch 3 training complete\n",
      "Accuracy on evaluation data: 9401 / 10000\n",
      "\n",
      "Epoch 4 training complete\n",
      "Accuracy on evaluation data: 9395 / 10000\n",
      "\n",
      "Epoch 5 training complete\n",
      "Accuracy on evaluation data: 9429 / 10000\n",
      "\n",
      "Epoch 6 training complete\n",
      "Accuracy on evaluation data: 9453 / 10000\n",
      "\n",
      "Epoch 7 training complete\n",
      "Accuracy on evaluation data: 9457 / 10000\n",
      "\n",
      "Epoch 8 training complete\n",
      "Accuracy on evaluation data: 9429 / 10000\n",
      "\n",
      "Epoch 9 training complete\n",
      "Accuracy on evaluation data: 9508 / 10000\n",
      "\n",
      "Epoch 10 training complete\n",
      "Accuracy on evaluation data: 9489 / 10000\n",
      "\n",
      "Epoch 11 training complete\n",
      "Accuracy on evaluation data: 9461 / 10000\n",
      "\n",
      "Epoch 12 training complete\n",
      "Accuracy on evaluation data: 9484 / 10000\n",
      "\n",
      "Epoch 13 training complete\n",
      "Accuracy on evaluation data: 9501 / 10000\n",
      "\n",
      "Epoch 14 training complete\n",
      "Accuracy on evaluation data: 9495 / 10000\n",
      "\n",
      "Epoch 15 training complete\n",
      "Accuracy on evaluation data: 9474 / 10000\n",
      "\n",
      "Epoch 16 training complete\n",
      "Accuracy on evaluation data: 9479 / 10000\n",
      "\n",
      "Epoch 17 training complete\n",
      "Accuracy on evaluation data: 9516 / 10000\n",
      "\n",
      "Epoch 18 training complete\n",
      "Accuracy on evaluation data: 9505 / 10000\n",
      "\n",
      "Epoch 19 training complete\n",
      "Accuracy on evaluation data: 9529 / 10000\n",
      "\n",
      "Epoch 20 training complete\n",
      "Accuracy on evaluation data: 9505 / 10000\n",
      "\n",
      "Epoch 21 training complete\n",
      "Accuracy on evaluation data: 9515 / 10000\n",
      "\n",
      "Epoch 22 training complete\n",
      "Accuracy on evaluation data: 9537 / 10000\n",
      "\n",
      "Epoch 23 training complete\n",
      "Accuracy on evaluation data: 9522 / 10000\n",
      "\n",
      "Epoch 24 training complete\n",
      "Accuracy on evaluation data: 9540 / 10000\n",
      "\n",
      "Epoch 25 training complete\n",
      "Accuracy on evaluation data: 9497 / 10000\n",
      "\n",
      "Epoch 26 training complete\n",
      "Accuracy on evaluation data: 9513 / 10000\n",
      "\n",
      "Epoch 27 training complete\n",
      "Accuracy on evaluation data: 9528 / 10000\n",
      "\n",
      "Epoch 28 training complete\n",
      "Accuracy on evaluation data: 9538 / 10000\n",
      "\n",
      "Epoch 29 training complete\n",
      "Accuracy on evaluation data: 9534 / 10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [9140,\n",
       "  9281,\n",
       "  9338,\n",
       "  9401,\n",
       "  9395,\n",
       "  9429,\n",
       "  9453,\n",
       "  9457,\n",
       "  9429,\n",
       "  9508,\n",
       "  9489,\n",
       "  9461,\n",
       "  9484,\n",
       "  9501,\n",
       "  9495,\n",
       "  9474,\n",
       "  9479,\n",
       "  9516,\n",
       "  9505,\n",
       "  9529,\n",
       "  9505,\n",
       "  9515,\n",
       "  9537,\n",
       "  9522,\n",
       "  9540,\n",
       "  9497,\n",
       "  9513,\n",
       "  9528,\n",
       "  9538,\n",
       "  9534],\n",
       " [],\n",
       " [])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import network2\n",
    "net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
    "net.large_weight_initializer()\n",
    "net.SGD(training_data, 30, 10, 0.5, evaluation_data = test_data, monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 3.02779775922\n",
      "Accuracy on training data: 649 / 1000\n",
      "Cost on evaluation data: 2.27788701259\n",
      "Accuracy on evaluation data: 5721 / 10000\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 2.5014329329\n",
      "Accuracy on training data: 786 / 1000\n",
      "Cost on evaluation data: 1.83963098024\n",
      "Accuracy on evaluation data: 6815 / 10000\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 2.23799639579\n",
      "Accuracy on training data: 849 / 1000\n",
      "Cost on evaluation data: 1.68894974486\n",
      "Accuracy on evaluation data: 7089 / 10000\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 2.02222123374\n",
      "Accuracy on training data: 893 / 1000\n",
      "Cost on evaluation data: 1.51808947361\n",
      "Accuracy on evaluation data: 7485 / 10000\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 1.89783855133\n",
      "Accuracy on training data: 916 / 1000\n",
      "Cost on evaluation data: 1.48054384008\n",
      "Accuracy on evaluation data: 7584 / 10000\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 1.77176609607\n",
      "Accuracy on training data: 933 / 1000\n",
      "Cost on evaluation data: 1.3867891186\n",
      "Accuracy on evaluation data: 7799 / 10000\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 1.72192269724\n",
      "Accuracy on training data: 940 / 1000\n",
      "Cost on evaluation data: 1.40425750425\n",
      "Accuracy on evaluation data: 7798 / 10000\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 1.63094477153\n",
      "Accuracy on training data: 947 / 1000\n",
      "Cost on evaluation data: 1.33390375875\n",
      "Accuracy on evaluation data: 7923 / 10000\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 1.57336295794\n",
      "Accuracy on training data: 952 / 1000\n",
      "Cost on evaluation data: 1.35899950176\n",
      "Accuracy on evaluation data: 7903 / 10000\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 1.5255304404\n",
      "Accuracy on training data: 960 / 1000\n",
      "Cost on evaluation data: 1.34589614382\n",
      "Accuracy on evaluation data: 7966 / 10000\n",
      "\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 1.48691198294\n",
      "Accuracy on training data: 963 / 1000\n",
      "Cost on evaluation data: 1.33643662704\n",
      "Accuracy on evaluation data: 8006 / 10000\n",
      "\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 1.42045358704\n",
      "Accuracy on training data: 970 / 1000\n",
      "Cost on evaluation data: 1.27198483176\n",
      "Accuracy on evaluation data: 8127 / 10000\n",
      "\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 1.38968545525\n",
      "Accuracy on training data: 977 / 1000\n",
      "Cost on evaluation data: 1.29128933396\n",
      "Accuracy on evaluation data: 8126 / 10000\n",
      "\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 1.35905003447\n",
      "Accuracy on training data: 979 / 1000\n",
      "Cost on evaluation data: 1.28554109383\n",
      "Accuracy on evaluation data: 8161 / 10000\n",
      "\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 1.33058633318\n",
      "Accuracy on training data: 982 / 1000\n",
      "Cost on evaluation data: 1.27905209372\n",
      "Accuracy on evaluation data: 8149 / 10000\n",
      "\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 1.29589651663\n",
      "Accuracy on training data: 988 / 1000\n",
      "Cost on evaluation data: 1.25952752253\n",
      "Accuracy on evaluation data: 8215 / 10000\n",
      "\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 1.26832924429\n",
      "Accuracy on training data: 989 / 1000\n",
      "Cost on evaluation data: 1.28049308312\n",
      "Accuracy on evaluation data: 8186 / 10000\n",
      "\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 1.2428937886\n",
      "Accuracy on training data: 992 / 1000\n",
      "Cost on evaluation data: 1.26266526403\n",
      "Accuracy on evaluation data: 8233 / 10000\n",
      "\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 1.22054115115\n",
      "Accuracy on training data: 992 / 1000\n",
      "Cost on evaluation data: 1.25369458406\n",
      "Accuracy on evaluation data: 8257 / 10000\n",
      "\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 1.20392843864\n",
      "Accuracy on training data: 991 / 1000\n",
      "Cost on evaluation data: 1.26055156165\n",
      "Accuracy on evaluation data: 8226 / 10000\n",
      "\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 1.18406611466\n",
      "Accuracy on training data: 992 / 1000\n",
      "Cost on evaluation data: 1.26835046069\n",
      "Accuracy on evaluation data: 8222 / 10000\n",
      "\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 1.16453156814\n",
      "Accuracy on training data: 992 / 1000\n",
      "Cost on evaluation data: 1.26959872947\n",
      "Accuracy on evaluation data: 8249 / 10000\n",
      "\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 1.14802790303\n",
      "Accuracy on training data: 994 / 1000\n",
      "Cost on evaluation data: 1.26040145881\n",
      "Accuracy on evaluation data: 8281 / 10000\n",
      "\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 1.13592088809\n",
      "Accuracy on training data: 993 / 1000\n",
      "Cost on evaluation data: 1.28566748633\n",
      "Accuracy on evaluation data: 8235 / 10000\n",
      "\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 1.11756826307\n",
      "Accuracy on training data: 994 / 1000\n",
      "Cost on evaluation data: 1.26002644981\n",
      "Accuracy on evaluation data: 8286 / 10000\n",
      "\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 1.10720167061\n",
      "Accuracy on training data: 994 / 1000\n",
      "Cost on evaluation data: 1.26693030257\n",
      "Accuracy on evaluation data: 8280 / 10000\n",
      "\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 1.09130294108\n",
      "Accuracy on training data: 994 / 1000\n",
      "Cost on evaluation data: 1.26358446674\n",
      "Accuracy on evaluation data: 8276 / 10000\n",
      "\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 1.07844752339\n",
      "Accuracy on training data: 994 / 1000\n",
      "Cost on evaluation data: 1.2742821114\n",
      "Accuracy on evaluation data: 8289 / 10000\n",
      "\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 1.06343159709\n",
      "Accuracy on training data: 994 / 1000\n",
      "Cost on evaluation data: 1.272046181\n",
      "Accuracy on evaluation data: 8287 / 10000\n",
      "\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 1.05326551495\n",
      "Accuracy on training data: 994 / 1000\n",
      "Cost on evaluation data: 1.26488502575\n",
      "Accuracy on evaluation data: 8307 / 10000\n",
      "\n",
      "Epoch 30 training complete\n",
      "Cost on training data: 1.04148466093\n",
      "Accuracy on training data: 995 / 1000\n",
      "Cost on evaluation data: 1.27045167422\n",
      "Accuracy on evaluation data: 8264 / 10000\n",
      "\n",
      "Epoch 31 training complete\n",
      "Cost on training data: 1.02496885985\n",
      "Accuracy on training data: 996 / 1000\n",
      "Cost on evaluation data: 1.26073449992\n",
      "Accuracy on evaluation data: 8319 / 10000\n",
      "\n",
      "Epoch 32 training complete\n",
      "Cost on training data: 1.01609333968\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 1.28966689597\n",
      "Accuracy on evaluation data: 8248 / 10000\n",
      "\n",
      "Epoch 33 training complete\n",
      "Cost on training data: 1.00175228932\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 1.25711395982\n",
      "Accuracy on evaluation data: 8322 / 10000\n",
      "\n",
      "Epoch 34 training complete\n",
      "Cost on training data: 0.990180040142\n",
      "Accuracy on training data: 998 / 1000\n",
      "Cost on evaluation data: 1.2738664728\n",
      "Accuracy on evaluation data: 8304 / 10000\n",
      "\n",
      "Epoch 35 training complete\n",
      "Cost on training data: 0.980730624112\n",
      "Accuracy on training data: 998 / 1000\n",
      "Cost on evaluation data: 1.2705707029\n",
      "Accuracy on evaluation data: 8326 / 10000\n",
      "\n",
      "Epoch 36 training complete\n",
      "Cost on training data: 0.970387889995\n",
      "Accuracy on training data: 998 / 1000\n",
      "Cost on evaluation data: 1.27400472962\n",
      "Accuracy on evaluation data: 8297 / 10000\n",
      "\n",
      "Epoch 37 training complete\n",
      "Cost on training data: 0.959525985376\n",
      "Accuracy on training data: 998 / 1000\n",
      "Cost on evaluation data: 1.2802451565\n",
      "Accuracy on evaluation data: 8295 / 10000\n",
      "\n",
      "Epoch 38 training complete\n",
      "Cost on training data: 0.949811727666\n",
      "Accuracy on training data: 998 / 1000\n",
      "Cost on evaluation data: 1.27579902502\n",
      "Accuracy on evaluation data: 8318 / 10000\n",
      "\n",
      "Epoch 39 training complete\n",
      "Cost on training data: 0.940751620717\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.25642592577\n",
      "Accuracy on evaluation data: 8328 / 10000\n",
      "\n",
      "Epoch 40 training complete\n",
      "Cost on training data: 0.930427069018\n",
      "Accuracy on training data: 998 / 1000\n",
      "Cost on evaluation data: 1.27216985289\n",
      "Accuracy on evaluation data: 8319 / 10000\n",
      "\n",
      "Epoch 41 training complete\n",
      "Cost on training data: 0.920932709855\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.28082858434\n",
      "Accuracy on evaluation data: 8292 / 10000\n",
      "\n",
      "Epoch 42 training complete\n",
      "Cost on training data: 0.911990603919\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.26249480523\n",
      "Accuracy on evaluation data: 8340 / 10000\n",
      "\n",
      "Epoch 43 training complete\n",
      "Cost on training data: 0.903250045884\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.26056861032\n",
      "Accuracy on evaluation data: 8349 / 10000\n",
      "\n",
      "Epoch 44 training complete\n",
      "Cost on training data: 0.894445169052\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2558704103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on evaluation data: 8352 / 10000\n",
      "\n",
      "Epoch 45 training complete\n",
      "Cost on training data: 0.885301397318\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.26971282665\n",
      "Accuracy on evaluation data: 8331 / 10000\n",
      "\n",
      "Epoch 46 training complete\n",
      "Cost on training data: 0.877157737151\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.26902295915\n",
      "Accuracy on evaluation data: 8337 / 10000\n",
      "\n",
      "Epoch 47 training complete\n",
      "Cost on training data: 0.868624320914\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.26234686436\n",
      "Accuracy on evaluation data: 8343 / 10000\n",
      "\n",
      "Epoch 48 training complete\n",
      "Cost on training data: 0.860597805361\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.27476912825\n",
      "Accuracy on evaluation data: 8335 / 10000\n",
      "\n",
      "Epoch 49 training complete\n",
      "Cost on training data: 0.852368456734\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.26285388274\n",
      "Accuracy on evaluation data: 8346 / 10000\n",
      "\n",
      "Epoch 50 training complete\n",
      "Cost on training data: 0.84594551525\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.25224134809\n",
      "Accuracy on evaluation data: 8352 / 10000\n",
      "\n",
      "Epoch 51 training complete\n",
      "Cost on training data: 0.836610785882\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.27011961905\n",
      "Accuracy on evaluation data: 8350 / 10000\n",
      "\n",
      "Epoch 52 training complete\n",
      "Cost on training data: 0.828561326657\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.26073512363\n",
      "Accuracy on evaluation data: 8377 / 10000\n",
      "\n",
      "Epoch 53 training complete\n",
      "Cost on training data: 0.821003248707\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.25734842301\n",
      "Accuracy on evaluation data: 8369 / 10000\n",
      "\n",
      "Epoch 54 training complete\n",
      "Cost on training data: 0.813450510374\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.24856236406\n",
      "Accuracy on evaluation data: 8388 / 10000\n",
      "\n",
      "Epoch 55 training complete\n",
      "Cost on training data: 0.805922779355\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.25134019344\n",
      "Accuracy on evaluation data: 8383 / 10000\n",
      "\n",
      "Epoch 56 training complete\n",
      "Cost on training data: 0.798532350072\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.26169802578\n",
      "Accuracy on evaluation data: 8357 / 10000\n",
      "\n",
      "Epoch 57 training complete\n",
      "Cost on training data: 0.791450588857\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.25406693761\n",
      "Accuracy on evaluation data: 8357 / 10000\n",
      "\n",
      "Epoch 58 training complete\n",
      "Cost on training data: 0.78389872123\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.25276831793\n",
      "Accuracy on evaluation data: 8370 / 10000\n",
      "\n",
      "Epoch 59 training complete\n",
      "Cost on training data: 0.777261864801\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.25513754086\n",
      "Accuracy on evaluation data: 8376 / 10000\n",
      "\n",
      "Epoch 60 training complete\n",
      "Cost on training data: 0.770564592086\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.24583657424\n",
      "Accuracy on evaluation data: 8383 / 10000\n",
      "\n",
      "Epoch 61 training complete\n",
      "Cost on training data: 0.76306114808\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.24347702775\n",
      "Accuracy on evaluation data: 8381 / 10000\n",
      "\n",
      "Epoch 62 training complete\n",
      "Cost on training data: 0.756180951387\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.25088327877\n",
      "Accuracy on evaluation data: 8374 / 10000\n",
      "\n",
      "Epoch 63 training complete\n",
      "Cost on training data: 0.749198540209\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.24583896107\n",
      "Accuracy on evaluation data: 8382 / 10000\n",
      "\n",
      "Epoch 64 training complete\n",
      "Cost on training data: 0.742946331121\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.24399800273\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "\n",
      "Epoch 65 training complete\n",
      "Cost on training data: 0.736598975799\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.25083743024\n",
      "Accuracy on evaluation data: 8380 / 10000\n",
      "\n",
      "Epoch 66 training complete\n",
      "Cost on training data: 0.729775250806\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.25128481402\n",
      "Accuracy on evaluation data: 8363 / 10000\n",
      "\n",
      "Epoch 67 training complete\n",
      "Cost on training data: 0.723854561201\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2433774666\n",
      "Accuracy on evaluation data: 8380 / 10000\n",
      "\n",
      "Epoch 68 training complete\n",
      "Cost on training data: 0.716736785468\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.24272786951\n",
      "Accuracy on evaluation data: 8386 / 10000\n",
      "\n",
      "Epoch 69 training complete\n",
      "Cost on training data: 0.710772873648\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.23459390112\n",
      "Accuracy on evaluation data: 8390 / 10000\n",
      "\n",
      "Epoch 70 training complete\n",
      "Cost on training data: 0.704198849649\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.23499828571\n",
      "Accuracy on evaluation data: 8400 / 10000\n",
      "\n",
      "Epoch 71 training complete\n",
      "Cost on training data: 0.698367054971\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.23711733321\n",
      "Accuracy on evaluation data: 8382 / 10000\n",
      "\n",
      "Epoch 72 training complete\n",
      "Cost on training data: 0.692110867305\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.23695970219\n",
      "Accuracy on evaluation data: 8373 / 10000\n",
      "\n",
      "Epoch 73 training complete\n",
      "Cost on training data: 0.685892278921\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.22888415478\n",
      "Accuracy on evaluation data: 8403 / 10000\n",
      "\n",
      "Epoch 74 training complete\n",
      "Cost on training data: 0.680323064249\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.23329643186\n",
      "Accuracy on evaluation data: 8396 / 10000\n",
      "\n",
      "Epoch 75 training complete\n",
      "Cost on training data: 0.674360831156\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.23242603722\n",
      "Accuracy on evaluation data: 8402 / 10000\n",
      "\n",
      "Epoch 76 training complete\n",
      "Cost on training data: 0.668532526405\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.23374476243\n",
      "Accuracy on evaluation data: 8406 / 10000\n",
      "\n",
      "Epoch 77 training complete\n",
      "Cost on training data: 0.662758124968\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2194352878\n",
      "Accuracy on evaluation data: 8401 / 10000\n",
      "\n",
      "Epoch 78 training complete\n",
      "Cost on training data: 0.656782851851\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.22512711598\n",
      "Accuracy on evaluation data: 8408 / 10000\n",
      "\n",
      "Epoch 79 training complete\n",
      "Cost on training data: 0.651132288894\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.22511516786\n",
      "Accuracy on evaluation data: 8402 / 10000\n",
      "\n",
      "Epoch 80 training complete\n",
      "Cost on training data: 0.645963183171\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.22938695628\n",
      "Accuracy on evaluation data: 8391 / 10000\n",
      "\n",
      "Epoch 81 training complete\n",
      "Cost on training data: 0.640204392398\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.22278511934\n",
      "Accuracy on evaluation data: 8391 / 10000\n",
      "\n",
      "Epoch 82 training complete\n",
      "Cost on training data: 0.634941040074\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.22479079976\n",
      "Accuracy on evaluation data: 8389 / 10000\n",
      "\n",
      "Epoch 83 training complete\n",
      "Cost on training data: 0.629667335601\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2228183397\n",
      "Accuracy on evaluation data: 8393 / 10000\n",
      "\n",
      "Epoch 84 training complete\n",
      "Cost on training data: 0.623848212172\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.21126144638\n",
      "Accuracy on evaluation data: 8407 / 10000\n",
      "\n",
      "Epoch 85 training complete\n",
      "Cost on training data: 0.618719607093\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.22026155778\n",
      "Accuracy on evaluation data: 8404 / 10000\n",
      "\n",
      "Epoch 86 training complete\n",
      "Cost on training data: 0.613318369547\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.206326553\n",
      "Accuracy on evaluation data: 8419 / 10000\n",
      "\n",
      "Epoch 87 training complete\n",
      "Cost on training data: 0.608180994224\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.21217981403\n",
      "Accuracy on evaluation data: 8403 / 10000\n",
      "\n",
      "Epoch 88 training complete\n",
      "Cost on training data: 0.603170619986\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.20178705715\n",
      "Accuracy on evaluation data: 8425 / 10000\n",
      "\n",
      "Epoch 89 training complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on training data: 0.597998328669\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.20066852028\n",
      "Accuracy on evaluation data: 8420 / 10000\n",
      "\n",
      "Epoch 90 training complete\n",
      "Cost on training data: 0.592999586301\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2034197644\n",
      "Accuracy on evaluation data: 8420 / 10000\n",
      "\n",
      "Epoch 91 training complete\n",
      "Cost on training data: 0.588063919707\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.19945961996\n",
      "Accuracy on evaluation data: 8424 / 10000\n",
      "\n",
      "Epoch 92 training complete\n",
      "Cost on training data: 0.58291948315\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.19666806042\n",
      "Accuracy on evaluation data: 8426 / 10000\n",
      "\n",
      "Epoch 93 training complete\n",
      "Cost on training data: 0.578156805311\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.19327933796\n",
      "Accuracy on evaluation data: 8433 / 10000\n",
      "\n",
      "Epoch 94 training complete\n",
      "Cost on training data: 0.573125586466\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.19257359694\n",
      "Accuracy on evaluation data: 8429 / 10000\n",
      "\n",
      "Epoch 95 training complete\n",
      "Cost on training data: 0.568351522051\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.19190393657\n",
      "Accuracy on evaluation data: 8435 / 10000\n",
      "\n",
      "Epoch 96 training complete\n",
      "Cost on training data: 0.563798164576\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.18929904487\n",
      "Accuracy on evaluation data: 8447 / 10000\n",
      "\n",
      "Epoch 97 training complete\n",
      "Cost on training data: 0.55902188513\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.19034236074\n",
      "Accuracy on evaluation data: 8428 / 10000\n",
      "\n",
      "Epoch 98 training complete\n",
      "Cost on training data: 0.554358404518\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1829317882\n",
      "Accuracy on evaluation data: 8441 / 10000\n",
      "\n",
      "Epoch 99 training complete\n",
      "Cost on training data: 0.549691087417\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.18113643832\n",
      "Accuracy on evaluation data: 8444 / 10000\n",
      "\n",
      "Epoch 100 training complete\n",
      "Cost on training data: 0.545343868886\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.18216911362\n",
      "Accuracy on evaluation data: 8438 / 10000\n",
      "\n",
      "Epoch 101 training complete\n",
      "Cost on training data: 0.540573249626\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.17833092095\n",
      "Accuracy on evaluation data: 8451 / 10000\n",
      "\n",
      "Epoch 102 training complete\n",
      "Cost on training data: 0.53647012435\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.17919475062\n",
      "Accuracy on evaluation data: 8458 / 10000\n",
      "\n",
      "Epoch 103 training complete\n",
      "Cost on training data: 0.531709109472\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.17262015512\n",
      "Accuracy on evaluation data: 8464 / 10000\n",
      "\n",
      "Epoch 104 training complete\n",
      "Cost on training data: 0.52738318215\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.17030787558\n",
      "Accuracy on evaluation data: 8453 / 10000\n",
      "\n",
      "Epoch 105 training complete\n",
      "Cost on training data: 0.523156203426\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1724816574\n",
      "Accuracy on evaluation data: 8460 / 10000\n",
      "\n",
      "Epoch 106 training complete\n",
      "Cost on training data: 0.518681151734\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.17209534782\n",
      "Accuracy on evaluation data: 8466 / 10000\n",
      "\n",
      "Epoch 107 training complete\n",
      "Cost on training data: 0.514419917609\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.17097518844\n",
      "Accuracy on evaluation data: 8470 / 10000\n",
      "\n",
      "Epoch 108 training complete\n",
      "Cost on training data: 0.510272941633\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.16424277204\n",
      "Accuracy on evaluation data: 8476 / 10000\n",
      "\n",
      "Epoch 109 training complete\n",
      "Cost on training data: 0.506163707514\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.16070553398\n",
      "Accuracy on evaluation data: 8473 / 10000\n",
      "\n",
      "Epoch 110 training complete\n",
      "Cost on training data: 0.502061675402\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.16353779095\n",
      "Accuracy on evaluation data: 8466 / 10000\n",
      "\n",
      "Epoch 111 training complete\n",
      "Cost on training data: 0.497907244786\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.16142625427\n",
      "Accuracy on evaluation data: 8469 / 10000\n",
      "\n",
      "Epoch 112 training complete\n",
      "Cost on training data: 0.493935745686\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.15317210801\n",
      "Accuracy on evaluation data: 8479 / 10000\n",
      "\n",
      "Epoch 113 training complete\n",
      "Cost on training data: 0.489890473219\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.15720239214\n",
      "Accuracy on evaluation data: 8481 / 10000\n",
      "\n",
      "Epoch 114 training complete\n",
      "Cost on training data: 0.486004632917\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1595730048\n",
      "Accuracy on evaluation data: 8476 / 10000\n",
      "\n",
      "Epoch 115 training complete\n",
      "Cost on training data: 0.481941478588\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.15415986982\n",
      "Accuracy on evaluation data: 8486 / 10000\n",
      "\n",
      "Epoch 116 training complete\n",
      "Cost on training data: 0.478182134789\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1524962939\n",
      "Accuracy on evaluation data: 8478 / 10000\n",
      "\n",
      "Epoch 117 training complete\n",
      "Cost on training data: 0.474201725298\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.14516925966\n",
      "Accuracy on evaluation data: 8496 / 10000\n",
      "\n",
      "Epoch 118 training complete\n",
      "Cost on training data: 0.470448376436\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.14192276896\n",
      "Accuracy on evaluation data: 8500 / 10000\n",
      "\n",
      "Epoch 119 training complete\n",
      "Cost on training data: 0.46662445685\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.14561888472\n",
      "Accuracy on evaluation data: 8488 / 10000\n",
      "\n",
      "Epoch 120 training complete\n",
      "Cost on training data: 0.463034021641\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1461413657\n",
      "Accuracy on evaluation data: 8481 / 10000\n",
      "\n",
      "Epoch 121 training complete\n",
      "Cost on training data: 0.459294875764\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.14211573311\n",
      "Accuracy on evaluation data: 8492 / 10000\n",
      "\n",
      "Epoch 122 training complete\n",
      "Cost on training data: 0.455626381219\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.13953981937\n",
      "Accuracy on evaluation data: 8490 / 10000\n",
      "\n",
      "Epoch 123 training complete\n",
      "Cost on training data: 0.452082415654\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.13029934708\n",
      "Accuracy on evaluation data: 8496 / 10000\n",
      "\n",
      "Epoch 124 training complete\n",
      "Cost on training data: 0.4486547515\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.14105997999\n",
      "Accuracy on evaluation data: 8478 / 10000\n",
      "\n",
      "Epoch 125 training complete\n",
      "Cost on training data: 0.444953684832\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.12608293109\n",
      "Accuracy on evaluation data: 8506 / 10000\n",
      "\n",
      "Epoch 126 training complete\n",
      "Cost on training data: 0.441409176808\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.12854120377\n",
      "Accuracy on evaluation data: 8499 / 10000\n",
      "\n",
      "Epoch 127 training complete\n",
      "Cost on training data: 0.438189436044\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1309704692\n",
      "Accuracy on evaluation data: 8494 / 10000\n",
      "\n",
      "Epoch 128 training complete\n",
      "Cost on training data: 0.434459202037\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.12628899669\n",
      "Accuracy on evaluation data: 8503 / 10000\n",
      "\n",
      "Epoch 129 training complete\n",
      "Cost on training data: 0.431161055848\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.12580094106\n",
      "Accuracy on evaluation data: 8492 / 10000\n",
      "\n",
      "Epoch 130 training complete\n",
      "Cost on training data: 0.427752337534\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.11731289822\n",
      "Accuracy on evaluation data: 8507 / 10000\n",
      "\n",
      "Epoch 131 training complete\n",
      "Cost on training data: 0.42460085453\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.11992060348\n",
      "Accuracy on evaluation data: 8509 / 10000\n",
      "\n",
      "Epoch 132 training complete\n",
      "Cost on training data: 0.421239939523\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1219255624\n",
      "Accuracy on evaluation data: 8503 / 10000\n",
      "\n",
      "Epoch 133 training complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on training data: 0.417877483898\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.11441494681\n",
      "Accuracy on evaluation data: 8510 / 10000\n",
      "\n",
      "Epoch 134 training complete\n",
      "Cost on training data: 0.41464414922\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.11580535296\n",
      "Accuracy on evaluation data: 8508 / 10000\n",
      "\n",
      "Epoch 135 training complete\n",
      "Cost on training data: 0.411416342991\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.10989709166\n",
      "Accuracy on evaluation data: 8514 / 10000\n",
      "\n",
      "Epoch 136 training complete\n",
      "Cost on training data: 0.408833514885\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.10888232904\n",
      "Accuracy on evaluation data: 8520 / 10000\n",
      "\n",
      "Epoch 137 training complete\n",
      "Cost on training data: 0.405156562258\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.10773476805\n",
      "Accuracy on evaluation data: 8523 / 10000\n",
      "\n",
      "Epoch 138 training complete\n",
      "Cost on training data: 0.402162804316\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.10849748367\n",
      "Accuracy on evaluation data: 8510 / 10000\n",
      "\n",
      "Epoch 139 training complete\n",
      "Cost on training data: 0.399074430243\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.10514669309\n",
      "Accuracy on evaluation data: 8520 / 10000\n",
      "\n",
      "Epoch 140 training complete\n",
      "Cost on training data: 0.396043099408\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.10243083803\n",
      "Accuracy on evaluation data: 8530 / 10000\n",
      "\n",
      "Epoch 141 training complete\n",
      "Cost on training data: 0.393096671343\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.10465490015\n",
      "Accuracy on evaluation data: 8513 / 10000\n",
      "\n",
      "Epoch 142 training complete\n",
      "Cost on training data: 0.390076937002\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.09729417359\n",
      "Accuracy on evaluation data: 8547 / 10000\n",
      "\n",
      "Epoch 143 training complete\n",
      "Cost on training data: 0.387047200989\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.09938898222\n",
      "Accuracy on evaluation data: 8526 / 10000\n",
      "\n",
      "Epoch 144 training complete\n",
      "Cost on training data: 0.384393210862\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.09791624982\n",
      "Accuracy on evaluation data: 8530 / 10000\n",
      "\n",
      "Epoch 145 training complete\n",
      "Cost on training data: 0.38130309531\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.09944607577\n",
      "Accuracy on evaluation data: 8514 / 10000\n",
      "\n",
      "Epoch 146 training complete\n",
      "Cost on training data: 0.378346700761\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.09637280093\n",
      "Accuracy on evaluation data: 8528 / 10000\n",
      "\n",
      "Epoch 147 training complete\n",
      "Cost on training data: 0.375568784574\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.08665323035\n",
      "Accuracy on evaluation data: 8550 / 10000\n",
      "\n",
      "Epoch 148 training complete\n",
      "Cost on training data: 0.372859649475\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.09182003037\n",
      "Accuracy on evaluation data: 8530 / 10000\n",
      "\n",
      "Epoch 149 training complete\n",
      "Cost on training data: 0.369987594934\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.08986081435\n",
      "Accuracy on evaluation data: 8532 / 10000\n",
      "\n",
      "Epoch 150 training complete\n",
      "Cost on training data: 0.367150820848\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.08302243772\n",
      "Accuracy on evaluation data: 8547 / 10000\n",
      "\n",
      "Epoch 151 training complete\n",
      "Cost on training data: 0.36445712484\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.07986286755\n",
      "Accuracy on evaluation data: 8554 / 10000\n",
      "\n",
      "Epoch 152 training complete\n",
      "Cost on training data: 0.361758368331\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.07980026619\n",
      "Accuracy on evaluation data: 8553 / 10000\n",
      "\n",
      "Epoch 153 training complete\n",
      "Cost on training data: 0.359068343248\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.08015685881\n",
      "Accuracy on evaluation data: 8548 / 10000\n",
      "\n",
      "Epoch 154 training complete\n",
      "Cost on training data: 0.356428271664\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.07205636531\n",
      "Accuracy on evaluation data: 8557 / 10000\n",
      "\n",
      "Epoch 155 training complete\n",
      "Cost on training data: 0.353914318294\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.07567971518\n",
      "Accuracy on evaluation data: 8556 / 10000\n",
      "\n",
      "Epoch 156 training complete\n",
      "Cost on training data: 0.351230552779\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.08015672507\n",
      "Accuracy on evaluation data: 8545 / 10000\n",
      "\n",
      "Epoch 157 training complete\n",
      "Cost on training data: 0.348740689658\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.07413223842\n",
      "Accuracy on evaluation data: 8549 / 10000\n",
      "\n",
      "Epoch 158 training complete\n",
      "Cost on training data: 0.346214141527\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.07595667002\n",
      "Accuracy on evaluation data: 8543 / 10000\n",
      "\n",
      "Epoch 159 training complete\n",
      "Cost on training data: 0.343656169288\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.06634364996\n",
      "Accuracy on evaluation data: 8563 / 10000\n",
      "\n",
      "Epoch 160 training complete\n",
      "Cost on training data: 0.341242077628\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0671540968\n",
      "Accuracy on evaluation data: 8554 / 10000\n",
      "\n",
      "Epoch 161 training complete\n",
      "Cost on training data: 0.338924126807\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.06655722055\n",
      "Accuracy on evaluation data: 8562 / 10000\n",
      "\n",
      "Epoch 162 training complete\n",
      "Cost on training data: 0.336327960254\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.06715143321\n",
      "Accuracy on evaluation data: 8556 / 10000\n",
      "\n",
      "Epoch 163 training complete\n",
      "Cost on training data: 0.334083385598\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.06117566604\n",
      "Accuracy on evaluation data: 8567 / 10000\n",
      "\n",
      "Epoch 164 training complete\n",
      "Cost on training data: 0.33162470153\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.06094423143\n",
      "Accuracy on evaluation data: 8553 / 10000\n",
      "\n",
      "Epoch 165 training complete\n",
      "Cost on training data: 0.329261148951\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.05534416354\n",
      "Accuracy on evaluation data: 8571 / 10000\n",
      "\n",
      "Epoch 166 training complete\n",
      "Cost on training data: 0.326866909093\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.05845248857\n",
      "Accuracy on evaluation data: 8568 / 10000\n",
      "\n",
      "Epoch 167 training complete\n",
      "Cost on training data: 0.324704336673\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.05598077442\n",
      "Accuracy on evaluation data: 8567 / 10000\n",
      "\n",
      "Epoch 168 training complete\n",
      "Cost on training data: 0.322473662242\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.06125985874\n",
      "Accuracy on evaluation data: 8566 / 10000\n",
      "\n",
      "Epoch 169 training complete\n",
      "Cost on training data: 0.319998630866\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.05898363968\n",
      "Accuracy on evaluation data: 8557 / 10000\n",
      "\n",
      "Epoch 170 training complete\n",
      "Cost on training data: 0.317799411645\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.05485911275\n",
      "Accuracy on evaluation data: 8566 / 10000\n",
      "\n",
      "Epoch 171 training complete\n",
      "Cost on training data: 0.315628849274\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.05222865957\n",
      "Accuracy on evaluation data: 8573 / 10000\n",
      "\n",
      "Epoch 172 training complete\n",
      "Cost on training data: 0.313362950925\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.04700089584\n",
      "Accuracy on evaluation data: 8578 / 10000\n",
      "\n",
      "Epoch 173 training complete\n",
      "Cost on training data: 0.311206251603\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.04241203169\n",
      "Accuracy on evaluation data: 8584 / 10000\n",
      "\n",
      "Epoch 174 training complete\n",
      "Cost on training data: 0.30896997253\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.04813610409\n",
      "Accuracy on evaluation data: 8574 / 10000\n",
      "\n",
      "Epoch 175 training complete\n",
      "Cost on training data: 0.307028606437\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.04299952415\n",
      "Accuracy on evaluation data: 8582 / 10000\n",
      "\n",
      "Epoch 176 training complete\n",
      "Cost on training data: 0.30479040742\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0409468244\n",
      "Accuracy on evaluation data: 8594 / 10000\n",
      "\n",
      "Epoch 177 training complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on training data: 0.302714953846\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.04392027053\n",
      "Accuracy on evaluation data: 8573 / 10000\n",
      "\n",
      "Epoch 178 training complete\n",
      "Cost on training data: 0.300764011458\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.03690438475\n",
      "Accuracy on evaluation data: 8591 / 10000\n",
      "\n",
      "Epoch 179 training complete\n",
      "Cost on training data: 0.298834039451\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.03693092387\n",
      "Accuracy on evaluation data: 8588 / 10000\n",
      "\n",
      "Epoch 180 training complete\n",
      "Cost on training data: 0.296621360623\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.03770136884\n",
      "Accuracy on evaluation data: 8586 / 10000\n",
      "\n",
      "Epoch 181 training complete\n",
      "Cost on training data: 0.294554445351\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.03432180039\n",
      "Accuracy on evaluation data: 8584 / 10000\n",
      "\n",
      "Epoch 182 training complete\n",
      "Cost on training data: 0.292626024755\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.03825315964\n",
      "Accuracy on evaluation data: 8579 / 10000\n",
      "\n",
      "Epoch 183 training complete\n",
      "Cost on training data: 0.290716017678\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.03592387218\n",
      "Accuracy on evaluation data: 8573 / 10000\n",
      "\n",
      "Epoch 184 training complete\n",
      "Cost on training data: 0.28879029609\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.02900748045\n",
      "Accuracy on evaluation data: 8594 / 10000\n",
      "\n",
      "Epoch 185 training complete\n",
      "Cost on training data: 0.286919375292\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.03564953886\n",
      "Accuracy on evaluation data: 8585 / 10000\n",
      "\n",
      "Epoch 186 training complete\n",
      "Cost on training data: 0.284917324203\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.02827137126\n",
      "Accuracy on evaluation data: 8586 / 10000\n",
      "\n",
      "Epoch 187 training complete\n",
      "Cost on training data: 0.282938365927\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0290859493\n",
      "Accuracy on evaluation data: 8587 / 10000\n",
      "\n",
      "Epoch 188 training complete\n",
      "Cost on training data: 0.281406014374\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0332780069\n",
      "Accuracy on evaluation data: 8576 / 10000\n",
      "\n",
      "Epoch 189 training complete\n",
      "Cost on training data: 0.27952894736\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.03044063957\n",
      "Accuracy on evaluation data: 8590 / 10000\n",
      "\n",
      "Epoch 190 training complete\n",
      "Cost on training data: 0.277526186395\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.02044401362\n",
      "Accuracy on evaluation data: 8601 / 10000\n",
      "\n",
      "Epoch 191 training complete\n",
      "Cost on training data: 0.275616862649\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.01949274853\n",
      "Accuracy on evaluation data: 8603 / 10000\n",
      "\n",
      "Epoch 192 training complete\n",
      "Cost on training data: 0.273853223845\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.02015725283\n",
      "Accuracy on evaluation data: 8602 / 10000\n",
      "\n",
      "Epoch 193 training complete\n",
      "Cost on training data: 0.271967476172\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.01474497713\n",
      "Accuracy on evaluation data: 8618 / 10000\n",
      "\n",
      "Epoch 194 training complete\n",
      "Cost on training data: 0.270398178953\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.02140212732\n",
      "Accuracy on evaluation data: 8589 / 10000\n",
      "\n",
      "Epoch 195 training complete\n",
      "Cost on training data: 0.268485738466\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.01644215765\n",
      "Accuracy on evaluation data: 8604 / 10000\n",
      "\n",
      "Epoch 196 training complete\n",
      "Cost on training data: 0.266869515063\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.01761119509\n",
      "Accuracy on evaluation data: 8594 / 10000\n",
      "\n",
      "Epoch 197 training complete\n",
      "Cost on training data: 0.265161964812\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0178085064\n",
      "Accuracy on evaluation data: 8605 / 10000\n",
      "\n",
      "Epoch 198 training complete\n",
      "Cost on training data: 0.263388327471\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.01330898784\n",
      "Accuracy on evaluation data: 8614 / 10000\n",
      "\n",
      "Epoch 199 training complete\n",
      "Cost on training data: 0.261800503837\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.01395257893\n",
      "Accuracy on evaluation data: 8599 / 10000\n",
      "\n",
      "Epoch 200 training complete\n",
      "Cost on training data: 0.26007805947\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.01015074902\n",
      "Accuracy on evaluation data: 8625 / 10000\n",
      "\n",
      "Epoch 201 training complete\n",
      "Cost on training data: 0.258529802547\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.01069928925\n",
      "Accuracy on evaluation data: 8620 / 10000\n",
      "\n",
      "Epoch 202 training complete\n",
      "Cost on training data: 0.256909594875\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.00911936809\n",
      "Accuracy on evaluation data: 8612 / 10000\n",
      "\n",
      "Epoch 203 training complete\n",
      "Cost on training data: 0.255312431825\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0040997879\n",
      "Accuracy on evaluation data: 8631 / 10000\n",
      "\n",
      "Epoch 204 training complete\n",
      "Cost on training data: 0.253563537539\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.00656751169\n",
      "Accuracy on evaluation data: 8618 / 10000\n",
      "\n",
      "Epoch 205 training complete\n",
      "Cost on training data: 0.252089526292\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.997165022134\n",
      "Accuracy on evaluation data: 8631 / 10000\n",
      "\n",
      "Epoch 206 training complete\n",
      "Cost on training data: 0.250518283283\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.997150573479\n",
      "Accuracy on evaluation data: 8638 / 10000\n",
      "\n",
      "Epoch 207 training complete\n",
      "Cost on training data: 0.248972083839\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.00801517735\n",
      "Accuracy on evaluation data: 8600 / 10000\n",
      "\n",
      "Epoch 208 training complete\n",
      "Cost on training data: 0.247374112063\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.00048170119\n",
      "Accuracy on evaluation data: 8620 / 10000\n",
      "\n",
      "Epoch 209 training complete\n",
      "Cost on training data: 0.245975570533\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.00154877242\n",
      "Accuracy on evaluation data: 8612 / 10000\n",
      "\n",
      "Epoch 210 training complete\n",
      "Cost on training data: 0.244365310217\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.995991983405\n",
      "Accuracy on evaluation data: 8629 / 10000\n",
      "\n",
      "Epoch 211 training complete\n",
      "Cost on training data: 0.243045048942\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.00244607132\n",
      "Accuracy on evaluation data: 8611 / 10000\n",
      "\n",
      "Epoch 212 training complete\n",
      "Cost on training data: 0.241437492981\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.996462644081\n",
      "Accuracy on evaluation data: 8621 / 10000\n",
      "\n",
      "Epoch 213 training complete\n",
      "Cost on training data: 0.240023840977\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.990194461915\n",
      "Accuracy on evaluation data: 8650 / 10000\n",
      "\n",
      "Epoch 214 training complete\n",
      "Cost on training data: 0.238509230902\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.998787447793\n",
      "Accuracy on evaluation data: 8612 / 10000\n",
      "\n",
      "Epoch 215 training complete\n",
      "Cost on training data: 0.237076211012\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.992352365038\n",
      "Accuracy on evaluation data: 8624 / 10000\n",
      "\n",
      "Epoch 216 training complete\n",
      "Cost on training data: 0.235935089202\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.990189132612\n",
      "Accuracy on evaluation data: 8638 / 10000\n",
      "\n",
      "Epoch 217 training complete\n",
      "Cost on training data: 0.234274250653\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.987882437554\n",
      "Accuracy on evaluation data: 8630 / 10000\n",
      "\n",
      "Epoch 218 training complete\n",
      "Cost on training data: 0.232811095272\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.985733410357\n",
      "Accuracy on evaluation data: 8634 / 10000\n",
      "\n",
      "Epoch 219 training complete\n",
      "Cost on training data: 0.231513472667\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.982161021016\n",
      "Accuracy on evaluation data: 8652 / 10000\n",
      "\n",
      "Epoch 220 training complete\n",
      "Cost on training data: 0.230376495138\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.981469506963\n",
      "Accuracy on evaluation data: 8637 / 10000\n",
      "\n",
      "Epoch 221 training complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on training data: 0.228966620985\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.985828188218\n",
      "Accuracy on evaluation data: 8631 / 10000\n",
      "\n",
      "Epoch 222 training complete\n",
      "Cost on training data: 0.227551474906\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.980555398009\n",
      "Accuracy on evaluation data: 8636 / 10000\n",
      "\n",
      "Epoch 223 training complete\n",
      "Cost on training data: 0.226280804533\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.980021721245\n",
      "Accuracy on evaluation data: 8648 / 10000\n",
      "\n",
      "Epoch 224 training complete\n",
      "Cost on training data: 0.224946758768\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.986171643089\n",
      "Accuracy on evaluation data: 8619 / 10000\n",
      "\n",
      "Epoch 225 training complete\n",
      "Cost on training data: 0.223580363717\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.985083112244\n",
      "Accuracy on evaluation data: 8635 / 10000\n",
      "\n",
      "Epoch 226 training complete\n",
      "Cost on training data: 0.222241839356\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.987020521229\n",
      "Accuracy on evaluation data: 8629 / 10000\n",
      "\n",
      "Epoch 227 training complete\n",
      "Cost on training data: 0.221100151325\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.983835197095\n",
      "Accuracy on evaluation data: 8638 / 10000\n",
      "\n",
      "Epoch 228 training complete\n",
      "Cost on training data: 0.219653038913\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.980968330032\n",
      "Accuracy on evaluation data: 8632 / 10000\n",
      "\n",
      "Epoch 229 training complete\n",
      "Cost on training data: 0.218675026783\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.984065496003\n",
      "Accuracy on evaluation data: 8619 / 10000\n",
      "\n",
      "Epoch 230 training complete\n",
      "Cost on training data: 0.217192605791\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.975204856964\n",
      "Accuracy on evaluation data: 8638 / 10000\n",
      "\n",
      "Epoch 231 training complete\n",
      "Cost on training data: 0.216110541369\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.969639847988\n",
      "Accuracy on evaluation data: 8649 / 10000\n",
      "\n",
      "Epoch 232 training complete\n",
      "Cost on training data: 0.21476719562\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.973347820041\n",
      "Accuracy on evaluation data: 8636 / 10000\n",
      "\n",
      "Epoch 233 training complete\n",
      "Cost on training data: 0.213706025903\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.976700801689\n",
      "Accuracy on evaluation data: 8643 / 10000\n",
      "\n",
      "Epoch 234 training complete\n",
      "Cost on training data: 0.212467337729\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.968395272113\n",
      "Accuracy on evaluation data: 8656 / 10000\n",
      "\n",
      "Epoch 235 training complete\n",
      "Cost on training data: 0.211134883967\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.971801492485\n",
      "Accuracy on evaluation data: 8655 / 10000\n",
      "\n",
      "Epoch 236 training complete\n",
      "Cost on training data: 0.210070835566\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.969412042247\n",
      "Accuracy on evaluation data: 8638 / 10000\n",
      "\n",
      "Epoch 237 training complete\n",
      "Cost on training data: 0.208835688927\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.969324080485\n",
      "Accuracy on evaluation data: 8640 / 10000\n",
      "\n",
      "Epoch 238 training complete\n",
      "Cost on training data: 0.207654760715\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.964668332854\n",
      "Accuracy on evaluation data: 8658 / 10000\n",
      "\n",
      "Epoch 239 training complete\n",
      "Cost on training data: 0.206741694961\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.965806689036\n",
      "Accuracy on evaluation data: 8657 / 10000\n",
      "\n",
      "Epoch 240 training complete\n",
      "Cost on training data: 0.205533471801\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.966858786612\n",
      "Accuracy on evaluation data: 8666 / 10000\n",
      "\n",
      "Epoch 241 training complete\n",
      "Cost on training data: 0.204606623566\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.968344337283\n",
      "Accuracy on evaluation data: 8659 / 10000\n",
      "\n",
      "Epoch 242 training complete\n",
      "Cost on training data: 0.203169664217\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.965907559614\n",
      "Accuracy on evaluation data: 8664 / 10000\n",
      "\n",
      "Epoch 243 training complete\n",
      "Cost on training data: 0.202018881073\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.961659739902\n",
      "Accuracy on evaluation data: 8662 / 10000\n",
      "\n",
      "Epoch 244 training complete\n",
      "Cost on training data: 0.201258957925\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.966759013828\n",
      "Accuracy on evaluation data: 8648 / 10000\n",
      "\n",
      "Epoch 245 training complete\n",
      "Cost on training data: 0.200042546095\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.960773110929\n",
      "Accuracy on evaluation data: 8655 / 10000\n",
      "\n",
      "Epoch 246 training complete\n",
      "Cost on training data: 0.198930322758\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.955876632637\n",
      "Accuracy on evaluation data: 8661 / 10000\n",
      "\n",
      "Epoch 247 training complete\n",
      "Cost on training data: 0.197764594275\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.956182167374\n",
      "Accuracy on evaluation data: 8673 / 10000\n",
      "\n",
      "Epoch 248 training complete\n",
      "Cost on training data: 0.196723063822\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.958607276256\n",
      "Accuracy on evaluation data: 8669 / 10000\n",
      "\n",
      "Epoch 249 training complete\n",
      "Cost on training data: 0.195735293191\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.953965695716\n",
      "Accuracy on evaluation data: 8677 / 10000\n",
      "\n",
      "Epoch 250 training complete\n",
      "Cost on training data: 0.19468368775\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.955125629077\n",
      "Accuracy on evaluation data: 8674 / 10000\n",
      "\n",
      "Epoch 251 training complete\n",
      "Cost on training data: 0.193801148537\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.958301711833\n",
      "Accuracy on evaluation data: 8665 / 10000\n",
      "\n",
      "Epoch 252 training complete\n",
      "Cost on training data: 0.192878673305\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.956673320184\n",
      "Accuracy on evaluation data: 8668 / 10000\n",
      "\n",
      "Epoch 253 training complete\n",
      "Cost on training data: 0.19167527021\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.952575448969\n",
      "Accuracy on evaluation data: 8679 / 10000\n",
      "\n",
      "Epoch 254 training complete\n",
      "Cost on training data: 0.190750259539\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.945907238474\n",
      "Accuracy on evaluation data: 8687 / 10000\n",
      "\n",
      "Epoch 255 training complete\n",
      "Cost on training data: 0.189795485915\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.957444806059\n",
      "Accuracy on evaluation data: 8664 / 10000\n",
      "\n",
      "Epoch 256 training complete\n",
      "Cost on training data: 0.189005732764\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.959993847556\n",
      "Accuracy on evaluation data: 8671 / 10000\n",
      "\n",
      "Epoch 257 training complete\n",
      "Cost on training data: 0.187974808807\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.952977959702\n",
      "Accuracy on evaluation data: 8672 / 10000\n",
      "\n",
      "Epoch 258 training complete\n",
      "Cost on training data: 0.186857675847\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.956542840184\n",
      "Accuracy on evaluation data: 8669 / 10000\n",
      "\n",
      "Epoch 259 training complete\n",
      "Cost on training data: 0.186111829625\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.94953538261\n",
      "Accuracy on evaluation data: 8673 / 10000\n",
      "\n",
      "Epoch 260 training complete\n",
      "Cost on training data: 0.185033622763\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.953261366866\n",
      "Accuracy on evaluation data: 8666 / 10000\n",
      "\n",
      "Epoch 261 training complete\n",
      "Cost on training data: 0.183981662255\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.947364580867\n",
      "Accuracy on evaluation data: 8681 / 10000\n",
      "\n",
      "Epoch 262 training complete\n",
      "Cost on training data: 0.18338383312\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.936451197853\n",
      "Accuracy on evaluation data: 8697 / 10000\n",
      "\n",
      "Epoch 263 training complete\n",
      "Cost on training data: 0.182214144423\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.948849591133\n",
      "Accuracy on evaluation data: 8684 / 10000\n",
      "\n",
      "Epoch 264 training complete\n",
      "Cost on training data: 0.181378539921\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.943292388518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on evaluation data: 8683 / 10000\n",
      "\n",
      "Epoch 265 training complete\n",
      "Cost on training data: 0.180481293242\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.948560720176\n",
      "Accuracy on evaluation data: 8683 / 10000\n",
      "\n",
      "Epoch 266 training complete\n",
      "Cost on training data: 0.179497264049\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.940283164296\n",
      "Accuracy on evaluation data: 8689 / 10000\n",
      "\n",
      "Epoch 267 training complete\n",
      "Cost on training data: 0.178691197297\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.94623631529\n",
      "Accuracy on evaluation data: 8683 / 10000\n",
      "\n",
      "Epoch 268 training complete\n",
      "Cost on training data: 0.177946349413\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.943864664575\n",
      "Accuracy on evaluation data: 8681 / 10000\n",
      "\n",
      "Epoch 269 training complete\n",
      "Cost on training data: 0.176947982558\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9380507423\n",
      "Accuracy on evaluation data: 8681 / 10000\n",
      "\n",
      "Epoch 270 training complete\n",
      "Cost on training data: 0.176144398731\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.943223423882\n",
      "Accuracy on evaluation data: 8697 / 10000\n",
      "\n",
      "Epoch 271 training complete\n",
      "Cost on training data: 0.175170487937\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.939540006308\n",
      "Accuracy on evaluation data: 8685 / 10000\n",
      "\n",
      "Epoch 272 training complete\n",
      "Cost on training data: 0.174663375899\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.931805203322\n",
      "Accuracy on evaluation data: 8694 / 10000\n",
      "\n",
      "Epoch 273 training complete\n",
      "Cost on training data: 0.173657385132\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.933030691036\n",
      "Accuracy on evaluation data: 8699 / 10000\n",
      "\n",
      "Epoch 274 training complete\n",
      "Cost on training data: 0.172841915215\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.934360659\n",
      "Accuracy on evaluation data: 8696 / 10000\n",
      "\n",
      "Epoch 275 training complete\n",
      "Cost on training data: 0.172226171819\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.942637719737\n",
      "Accuracy on evaluation data: 8686 / 10000\n",
      "\n",
      "Epoch 276 training complete\n",
      "Cost on training data: 0.171175734227\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.936359861931\n",
      "Accuracy on evaluation data: 8691 / 10000\n",
      "\n",
      "Epoch 277 training complete\n",
      "Cost on training data: 0.170477367748\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.941711116627\n",
      "Accuracy on evaluation data: 8683 / 10000\n",
      "\n",
      "Epoch 278 training complete\n",
      "Cost on training data: 0.169693919852\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.928925433269\n",
      "Accuracy on evaluation data: 8690 / 10000\n",
      "\n",
      "Epoch 279 training complete\n",
      "Cost on training data: 0.168871188421\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.933906438185\n",
      "Accuracy on evaluation data: 8699 / 10000\n",
      "\n",
      "Epoch 280 training complete\n",
      "Cost on training data: 0.168081644072\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.935336274448\n",
      "Accuracy on evaluation data: 8680 / 10000\n",
      "\n",
      "Epoch 281 training complete\n",
      "Cost on training data: 0.167745659772\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.942492362119\n",
      "Accuracy on evaluation data: 8681 / 10000\n",
      "\n",
      "Epoch 282 training complete\n",
      "Cost on training data: 0.16666798549\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.929391181008\n",
      "Accuracy on evaluation data: 8695 / 10000\n",
      "\n",
      "Epoch 283 training complete\n",
      "Cost on training data: 0.165904690459\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.927249121737\n",
      "Accuracy on evaluation data: 8695 / 10000\n",
      "\n",
      "Epoch 284 training complete\n",
      "Cost on training data: 0.165117982133\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.925183921213\n",
      "Accuracy on evaluation data: 8708 / 10000\n",
      "\n",
      "Epoch 285 training complete\n",
      "Cost on training data: 0.164432106904\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.933584220095\n",
      "Accuracy on evaluation data: 8687 / 10000\n",
      "\n",
      "Epoch 286 training complete\n",
      "Cost on training data: 0.163934620543\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.925988198006\n",
      "Accuracy on evaluation data: 8705 / 10000\n",
      "\n",
      "Epoch 287 training complete\n",
      "Cost on training data: 0.162889953901\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.924793729013\n",
      "Accuracy on evaluation data: 8693 / 10000\n",
      "\n",
      "Epoch 288 training complete\n",
      "Cost on training data: 0.162260009764\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.931841695274\n",
      "Accuracy on evaluation data: 8687 / 10000\n",
      "\n",
      "Epoch 289 training complete\n",
      "Cost on training data: 0.161452188172\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.924247235883\n",
      "Accuracy on evaluation data: 8711 / 10000\n",
      "\n",
      "Epoch 290 training complete\n",
      "Cost on training data: 0.160820793717\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.926217802382\n",
      "Accuracy on evaluation data: 8704 / 10000\n",
      "\n",
      "Epoch 291 training complete\n",
      "Cost on training data: 0.16016810196\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.920848190846\n",
      "Accuracy on evaluation data: 8702 / 10000\n",
      "\n",
      "Epoch 292 training complete\n",
      "Cost on training data: 0.159473001577\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.930053303882\n",
      "Accuracy on evaluation data: 8695 / 10000\n",
      "\n",
      "Epoch 293 training complete\n",
      "Cost on training data: 0.15872352564\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.924656094559\n",
      "Accuracy on evaluation data: 8691 / 10000\n",
      "\n",
      "Epoch 294 training complete\n",
      "Cost on training data: 0.158071962193\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.923693076372\n",
      "Accuracy on evaluation data: 8699 / 10000\n",
      "\n",
      "Epoch 295 training complete\n",
      "Cost on training data: 0.157661476535\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.926502337943\n",
      "Accuracy on evaluation data: 8689 / 10000\n",
      "\n",
      "Epoch 296 training complete\n",
      "Cost on training data: 0.1568942565\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.918259702807\n",
      "Accuracy on evaluation data: 8712 / 10000\n",
      "\n",
      "Epoch 297 training complete\n",
      "Cost on training data: 0.156072742252\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.920194047854\n",
      "Accuracy on evaluation data: 8704 / 10000\n",
      "\n",
      "Epoch 298 training complete\n",
      "Cost on training data: 0.155314344761\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.919564526956\n",
      "Accuracy on evaluation data: 8703 / 10000\n",
      "\n",
      "Epoch 299 training complete\n",
      "Cost on training data: 0.15470365322\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.918509871829\n",
      "Accuracy on evaluation data: 8701 / 10000\n",
      "\n",
      "Epoch 300 training complete\n",
      "Cost on training data: 0.154104182722\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.920952375508\n",
      "Accuracy on evaluation data: 8693 / 10000\n",
      "\n",
      "Epoch 301 training complete\n",
      "Cost on training data: 0.153399241131\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.916692600468\n",
      "Accuracy on evaluation data: 8706 / 10000\n",
      "\n",
      "Epoch 302 training complete\n",
      "Cost on training data: 0.152956458549\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.912769935671\n",
      "Accuracy on evaluation data: 8713 / 10000\n",
      "\n",
      "Epoch 303 training complete\n",
      "Cost on training data: 0.152136058186\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.916832074217\n",
      "Accuracy on evaluation data: 8701 / 10000\n",
      "\n",
      "Epoch 304 training complete\n",
      "Cost on training data: 0.151613454725\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.923932274607\n",
      "Accuracy on evaluation data: 8701 / 10000\n",
      "\n",
      "Epoch 305 training complete\n",
      "Cost on training data: 0.151029392895\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.915819138007\n",
      "Accuracy on evaluation data: 8707 / 10000\n",
      "\n",
      "Epoch 306 training complete\n",
      "Cost on training data: 0.150339332196\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.918620820356\n",
      "Accuracy on evaluation data: 8703 / 10000\n",
      "\n",
      "Epoch 307 training complete\n",
      "Cost on training data: 0.14991601862\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.914296709761\n",
      "Accuracy on evaluation data: 8716 / 10000\n",
      "\n",
      "Epoch 308 training complete\n",
      "Cost on training data: 0.149180887822\n",
      "Accuracy on training data: 1000 / 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on evaluation data: 0.914430235642\n",
      "Accuracy on evaluation data: 8710 / 10000\n",
      "\n",
      "Epoch 309 training complete\n",
      "Cost on training data: 0.148503639813\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.918469641625\n",
      "Accuracy on evaluation data: 8708 / 10000\n",
      "\n",
      "Epoch 310 training complete\n",
      "Cost on training data: 0.147994773754\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.919994851233\n",
      "Accuracy on evaluation data: 8697 / 10000\n",
      "\n",
      "Epoch 311 training complete\n",
      "Cost on training data: 0.147563089534\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.916417435968\n",
      "Accuracy on evaluation data: 8696 / 10000\n",
      "\n",
      "Epoch 312 training complete\n",
      "Cost on training data: 0.146823569716\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.919663573822\n",
      "Accuracy on evaluation data: 8691 / 10000\n",
      "\n",
      "Epoch 313 training complete\n",
      "Cost on training data: 0.146271736586\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.911660587068\n",
      "Accuracy on evaluation data: 8705 / 10000\n",
      "\n",
      "Epoch 314 training complete\n",
      "Cost on training data: 0.145688222611\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.91272290499\n",
      "Accuracy on evaluation data: 8708 / 10000\n",
      "\n",
      "Epoch 315 training complete\n",
      "Cost on training data: 0.145180917417\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.918021567848\n",
      "Accuracy on evaluation data: 8703 / 10000\n",
      "\n",
      "Epoch 316 training complete\n",
      "Cost on training data: 0.144784510425\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.910328292522\n",
      "Accuracy on evaluation data: 8707 / 10000\n",
      "\n",
      "Epoch 317 training complete\n",
      "Cost on training data: 0.144102214748\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.91126950143\n",
      "Accuracy on evaluation data: 8705 / 10000\n",
      "\n",
      "Epoch 318 training complete\n",
      "Cost on training data: 0.143567660952\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.917419194606\n",
      "Accuracy on evaluation data: 8709 / 10000\n",
      "\n",
      "Epoch 319 training complete\n",
      "Cost on training data: 0.143077650138\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.917638676099\n",
      "Accuracy on evaluation data: 8697 / 10000\n",
      "\n",
      "Epoch 320 training complete\n",
      "Cost on training data: 0.142505403112\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.911041929757\n",
      "Accuracy on evaluation data: 8714 / 10000\n",
      "\n",
      "Epoch 321 training complete\n",
      "Cost on training data: 0.142334378951\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.910352430901\n",
      "Accuracy on evaluation data: 8709 / 10000\n",
      "\n",
      "Epoch 322 training complete\n",
      "Cost on training data: 0.14170917567\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.913368688392\n",
      "Accuracy on evaluation data: 8710 / 10000\n",
      "\n",
      "Epoch 323 training complete\n",
      "Cost on training data: 0.141132594516\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.911978192482\n",
      "Accuracy on evaluation data: 8707 / 10000\n",
      "\n",
      "Epoch 324 training complete\n",
      "Cost on training data: 0.140504063422\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.911650647636\n",
      "Accuracy on evaluation data: 8716 / 10000\n",
      "\n",
      "Epoch 325 training complete\n",
      "Cost on training data: 0.1398784977\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.909256417485\n",
      "Accuracy on evaluation data: 8709 / 10000\n",
      "\n",
      "Epoch 326 training complete\n",
      "Cost on training data: 0.139597456005\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.91135508712\n",
      "Accuracy on evaluation data: 8719 / 10000\n",
      "\n",
      "Epoch 327 training complete\n",
      "Cost on training data: 0.138958593507\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.906876875686\n",
      "Accuracy on evaluation data: 8705 / 10000\n",
      "\n",
      "Epoch 328 training complete\n",
      "Cost on training data: 0.138464194155\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.906771687766\n",
      "Accuracy on evaluation data: 8710 / 10000\n",
      "\n",
      "Epoch 329 training complete\n",
      "Cost on training data: 0.137973264015\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.902689641676\n",
      "Accuracy on evaluation data: 8713 / 10000\n",
      "\n",
      "Epoch 330 training complete\n",
      "Cost on training data: 0.137585155247\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.903399309421\n",
      "Accuracy on evaluation data: 8711 / 10000\n",
      "\n",
      "Epoch 331 training complete\n",
      "Cost on training data: 0.137079295025\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.900196622828\n",
      "Accuracy on evaluation data: 8707 / 10000\n",
      "\n",
      "Epoch 332 training complete\n",
      "Cost on training data: 0.136628684134\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.904943797217\n",
      "Accuracy on evaluation data: 8707 / 10000\n",
      "\n",
      "Epoch 333 training complete\n",
      "Cost on training data: 0.136221354656\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.907037799955\n",
      "Accuracy on evaluation data: 8701 / 10000\n",
      "\n",
      "Epoch 334 training complete\n",
      "Cost on training data: 0.135670869933\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.905011319766\n",
      "Accuracy on evaluation data: 8710 / 10000\n",
      "\n",
      "Epoch 335 training complete\n",
      "Cost on training data: 0.135349505191\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.903998706162\n",
      "Accuracy on evaluation data: 8705 / 10000\n",
      "\n",
      "Epoch 336 training complete\n",
      "Cost on training data: 0.134950624421\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.900114882817\n",
      "Accuracy on evaluation data: 8724 / 10000\n",
      "\n",
      "Epoch 337 training complete\n",
      "Cost on training data: 0.134374614691\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.903248226118\n",
      "Accuracy on evaluation data: 8699 / 10000\n",
      "\n",
      "Epoch 338 training complete\n",
      "Cost on training data: 0.13415942755\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.916148243908\n",
      "Accuracy on evaluation data: 8695 / 10000\n",
      "\n",
      "Epoch 339 training complete\n",
      "Cost on training data: 0.13352789494\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.904011688367\n",
      "Accuracy on evaluation data: 8703 / 10000\n",
      "\n",
      "Epoch 340 training complete\n",
      "Cost on training data: 0.133157573503\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.896742392986\n",
      "Accuracy on evaluation data: 8717 / 10000\n",
      "\n",
      "Epoch 341 training complete\n",
      "Cost on training data: 0.132737757579\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.902343311497\n",
      "Accuracy on evaluation data: 8715 / 10000\n",
      "\n",
      "Epoch 342 training complete\n",
      "Cost on training data: 0.132320143095\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.908306259211\n",
      "Accuracy on evaluation data: 8701 / 10000\n",
      "\n",
      "Epoch 343 training complete\n",
      "Cost on training data: 0.131879913192\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.907067264021\n",
      "Accuracy on evaluation data: 8710 / 10000\n",
      "\n",
      "Epoch 344 training complete\n",
      "Cost on training data: 0.131776093451\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.897591757608\n",
      "Accuracy on evaluation data: 8710 / 10000\n",
      "\n",
      "Epoch 345 training complete\n",
      "Cost on training data: 0.131077685892\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.902494840648\n",
      "Accuracy on evaluation data: 8716 / 10000\n",
      "\n",
      "Epoch 346 training complete\n",
      "Cost on training data: 0.130621459287\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.895470913752\n",
      "Accuracy on evaluation data: 8707 / 10000\n",
      "\n",
      "Epoch 347 training complete\n",
      "Cost on training data: 0.130141300044\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.899724701057\n",
      "Accuracy on evaluation data: 8705 / 10000\n",
      "\n",
      "Epoch 348 training complete\n",
      "Cost on training data: 0.130065069206\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.902073400913\n",
      "Accuracy on evaluation data: 8714 / 10000\n",
      "\n",
      "Epoch 349 training complete\n",
      "Cost on training data: 0.129402802598\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.897734262093\n",
      "Accuracy on evaluation data: 8720 / 10000\n",
      "\n",
      "Epoch 350 training complete\n",
      "Cost on training data: 0.129068153807\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.902815410672\n",
      "Accuracy on evaluation data: 8711 / 10000\n",
      "\n",
      "Epoch 351 training complete\n",
      "Cost on training data: 0.128612000315\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.895621253685\n",
      "Accuracy on evaluation data: 8723 / 10000\n",
      "\n",
      "Epoch 352 training complete\n",
      "Cost on training data: 0.128197543953\n",
      "Accuracy on training data: 1000 / 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on evaluation data: 0.893800109251\n",
      "Accuracy on evaluation data: 8721 / 10000\n",
      "\n",
      "Epoch 353 training complete\n",
      "Cost on training data: 0.12784640701\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.893902363622\n",
      "Accuracy on evaluation data: 8716 / 10000\n",
      "\n",
      "Epoch 354 training complete\n",
      "Cost on training data: 0.127464703194\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.899831970371\n",
      "Accuracy on evaluation data: 8701 / 10000\n",
      "\n",
      "Epoch 355 training complete\n",
      "Cost on training data: 0.127050725185\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.903581997163\n",
      "Accuracy on evaluation data: 8710 / 10000\n",
      "\n",
      "Epoch 356 training complete\n",
      "Cost on training data: 0.126637404081\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.897681832966\n",
      "Accuracy on evaluation data: 8710 / 10000\n",
      "\n",
      "Epoch 357 training complete\n",
      "Cost on training data: 0.126540053487\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.91170361215\n",
      "Accuracy on evaluation data: 8702 / 10000\n",
      "\n",
      "Epoch 358 training complete\n",
      "Cost on training data: 0.125860047027\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.895874244539\n",
      "Accuracy on evaluation data: 8716 / 10000\n",
      "\n",
      "Epoch 359 training complete\n",
      "Cost on training data: 0.125652969643\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.892647889314\n",
      "Accuracy on evaluation data: 8714 / 10000\n",
      "\n",
      "Epoch 360 training complete\n",
      "Cost on training data: 0.125250982143\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.899146686169\n",
      "Accuracy on evaluation data: 8720 / 10000\n",
      "\n",
      "Epoch 361 training complete\n",
      "Cost on training data: 0.125125429453\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.901991352068\n",
      "Accuracy on evaluation data: 8706 / 10000\n",
      "\n",
      "Epoch 362 training complete\n",
      "Cost on training data: 0.124651075726\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.899720140206\n",
      "Accuracy on evaluation data: 8712 / 10000\n",
      "\n",
      "Epoch 363 training complete\n",
      "Cost on training data: 0.12434553916\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.896812423238\n",
      "Accuracy on evaluation data: 8710 / 10000\n",
      "\n",
      "Epoch 364 training complete\n",
      "Cost on training data: 0.123849532061\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.892569921863\n",
      "Accuracy on evaluation data: 8714 / 10000\n",
      "\n",
      "Epoch 365 training complete\n",
      "Cost on training data: 0.123575530275\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.895482676912\n",
      "Accuracy on evaluation data: 8722 / 10000\n",
      "\n",
      "Epoch 366 training complete\n",
      "Cost on training data: 0.123156491305\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.890312560276\n",
      "Accuracy on evaluation data: 8710 / 10000\n",
      "\n",
      "Epoch 367 training complete\n",
      "Cost on training data: 0.122872395521\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.898950185401\n",
      "Accuracy on evaluation data: 8712 / 10000\n",
      "\n",
      "Epoch 368 training complete\n",
      "Cost on training data: 0.122568668677\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.892140156351\n",
      "Accuracy on evaluation data: 8715 / 10000\n",
      "\n",
      "Epoch 369 training complete\n",
      "Cost on training data: 0.122208091275\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.902974189728\n",
      "Accuracy on evaluation data: 8712 / 10000\n",
      "\n",
      "Epoch 370 training complete\n",
      "Cost on training data: 0.121773129774\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.892552150544\n",
      "Accuracy on evaluation data: 8709 / 10000\n",
      "\n",
      "Epoch 371 training complete\n",
      "Cost on training data: 0.121682438414\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.894479859102\n",
      "Accuracy on evaluation data: 8719 / 10000\n",
      "\n",
      "Epoch 372 training complete\n",
      "Cost on training data: 0.121241166394\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.88911410448\n",
      "Accuracy on evaluation data: 8726 / 10000\n",
      "\n",
      "Epoch 373 training complete\n",
      "Cost on training data: 0.120903811547\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.889371289315\n",
      "Accuracy on evaluation data: 8724 / 10000\n",
      "\n",
      "Epoch 374 training complete\n",
      "Cost on training data: 0.120512486286\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.895468880294\n",
      "Accuracy on evaluation data: 8715 / 10000\n",
      "\n",
      "Epoch 375 training complete\n",
      "Cost on training data: 0.120207074445\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.893575395287\n",
      "Accuracy on evaluation data: 8724 / 10000\n",
      "\n",
      "Epoch 376 training complete\n",
      "Cost on training data: 0.11989831855\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.891854197653\n",
      "Accuracy on evaluation data: 8712 / 10000\n",
      "\n",
      "Epoch 377 training complete\n",
      "Cost on training data: 0.119680680066\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.892878933939\n",
      "Accuracy on evaluation data: 8717 / 10000\n",
      "\n",
      "Epoch 378 training complete\n",
      "Cost on training data: 0.119382163904\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.888241989002\n",
      "Accuracy on evaluation data: 8721 / 10000\n",
      "\n",
      "Epoch 379 training complete\n",
      "Cost on training data: 0.119037328993\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.894285393405\n",
      "Accuracy on evaluation data: 8714 / 10000\n",
      "\n",
      "Epoch 380 training complete\n",
      "Cost on training data: 0.118819587891\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8949414266\n",
      "Accuracy on evaluation data: 8715 / 10000\n",
      "\n",
      "Epoch 381 training complete\n",
      "Cost on training data: 0.118388571968\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.890087003482\n",
      "Accuracy on evaluation data: 8723 / 10000\n",
      "\n",
      "Epoch 382 training complete\n",
      "Cost on training data: 0.118164452128\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.885659712181\n",
      "Accuracy on evaluation data: 8717 / 10000\n",
      "\n",
      "Epoch 383 training complete\n",
      "Cost on training data: 0.11801677488\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.886618799496\n",
      "Accuracy on evaluation data: 8723 / 10000\n",
      "\n",
      "Epoch 384 training complete\n",
      "Cost on training data: 0.117662661951\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.890266617854\n",
      "Accuracy on evaluation data: 8711 / 10000\n",
      "\n",
      "Epoch 385 training complete\n",
      "Cost on training data: 0.117463994232\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.887590310301\n",
      "Accuracy on evaluation data: 8720 / 10000\n",
      "\n",
      "Epoch 386 training complete\n",
      "Cost on training data: 0.117304400669\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.894704528815\n",
      "Accuracy on evaluation data: 8713 / 10000\n",
      "\n",
      "Epoch 387 training complete\n",
      "Cost on training data: 0.116768584059\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8930667259\n",
      "Accuracy on evaluation data: 8707 / 10000\n",
      "\n",
      "Epoch 388 training complete\n",
      "Cost on training data: 0.116640378684\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.889499723532\n",
      "Accuracy on evaluation data: 8718 / 10000\n",
      "\n",
      "Epoch 389 training complete\n",
      "Cost on training data: 0.116143863213\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.886627328676\n",
      "Accuracy on evaluation data: 8722 / 10000\n",
      "\n",
      "Epoch 390 training complete\n",
      "Cost on training data: 0.116060746966\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.882724401673\n",
      "Accuracy on evaluation data: 8715 / 10000\n",
      "\n",
      "Epoch 391 training complete\n",
      "Cost on training data: 0.11563566166\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.886331933394\n",
      "Accuracy on evaluation data: 8725 / 10000\n",
      "\n",
      "Epoch 392 training complete\n",
      "Cost on training data: 0.115443959995\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.887700982674\n",
      "Accuracy on evaluation data: 8717 / 10000\n",
      "\n",
      "Epoch 393 training complete\n",
      "Cost on training data: 0.115267124772\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.883935695712\n",
      "Accuracy on evaluation data: 8718 / 10000\n",
      "\n",
      "Epoch 394 training complete\n",
      "Cost on training data: 0.1151955397\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.896060570788\n",
      "Accuracy on evaluation data: 8714 / 10000\n",
      "\n",
      "Epoch 395 training complete\n",
      "Cost on training data: 0.114768423192\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.891500131846\n",
      "Accuracy on evaluation data: 8717 / 10000\n",
      "\n",
      "Epoch 396 training complete\n",
      "Cost on training data: 0.114523972986\n",
      "Accuracy on training data: 1000 / 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on evaluation data: 0.884493230845\n",
      "Accuracy on evaluation data: 8722 / 10000\n",
      "\n",
      "Epoch 397 training complete\n",
      "Cost on training data: 0.114500596213\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.889482584248\n",
      "Accuracy on evaluation data: 8713 / 10000\n",
      "\n",
      "Epoch 398 training complete\n",
      "Cost on training data: 0.113953915166\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.889002873837\n",
      "Accuracy on evaluation data: 8724 / 10000\n",
      "\n",
      "Epoch 399 training complete\n",
      "Cost on training data: 0.113570405584\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.887958633424\n",
      "Accuracy on evaluation data: 8720 / 10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2.2778870125876165,\n",
       "  1.8396309802428779,\n",
       "  1.6889497448603528,\n",
       "  1.5180894736062067,\n",
       "  1.4805438400804056,\n",
       "  1.3867891185957082,\n",
       "  1.4042575042532253,\n",
       "  1.3339037587475548,\n",
       "  1.3589995017630365,\n",
       "  1.3458961438216057,\n",
       "  1.3364366270354298,\n",
       "  1.271984831758858,\n",
       "  1.2912893339645097,\n",
       "  1.285541093834311,\n",
       "  1.2790520937188303,\n",
       "  1.2595275225345,\n",
       "  1.2804930831237549,\n",
       "  1.2626652640334137,\n",
       "  1.253694584062254,\n",
       "  1.2605515616490899,\n",
       "  1.2683504606856513,\n",
       "  1.2695987294682611,\n",
       "  1.2604014588058219,\n",
       "  1.285667486327906,\n",
       "  1.2600264498123903,\n",
       "  1.2669303025740342,\n",
       "  1.2635844667415728,\n",
       "  1.2742821113961145,\n",
       "  1.272046181001128,\n",
       "  1.2648850257472615,\n",
       "  1.2704516742172576,\n",
       "  1.2607344999245296,\n",
       "  1.2896668959709083,\n",
       "  1.2571139598247383,\n",
       "  1.2738664727986255,\n",
       "  1.270570702897971,\n",
       "  1.2740047296209032,\n",
       "  1.2802451565004742,\n",
       "  1.275799025023194,\n",
       "  1.2564259257702028,\n",
       "  1.272169852885367,\n",
       "  1.2808285843378417,\n",
       "  1.2624948052333285,\n",
       "  1.2605686103249099,\n",
       "  1.2558704102967526,\n",
       "  1.2697128266511493,\n",
       "  1.2690229591486857,\n",
       "  1.2623468643619438,\n",
       "  1.2747691282532638,\n",
       "  1.2628538827354405,\n",
       "  1.2522413480902517,\n",
       "  1.270119619051799,\n",
       "  1.2607351236315545,\n",
       "  1.2573484230053693,\n",
       "  1.2485623640637835,\n",
       "  1.2513401934369885,\n",
       "  1.2616980257819268,\n",
       "  1.2540669376059108,\n",
       "  1.2527683179343507,\n",
       "  1.2551375408582615,\n",
       "  1.2458365742391655,\n",
       "  1.2434770277500442,\n",
       "  1.2508832787748148,\n",
       "  1.245838961066897,\n",
       "  1.243998002730674,\n",
       "  1.2508374302414658,\n",
       "  1.2512848140236539,\n",
       "  1.2433774666033939,\n",
       "  1.2427278695139454,\n",
       "  1.2345939011192217,\n",
       "  1.2349982857053643,\n",
       "  1.237117333209349,\n",
       "  1.2369597021934662,\n",
       "  1.2288841547804141,\n",
       "  1.233296431856851,\n",
       "  1.232426037215764,\n",
       "  1.233744762430538,\n",
       "  1.2194352878037396,\n",
       "  1.2251271159779704,\n",
       "  1.2251151678573218,\n",
       "  1.2293869562833595,\n",
       "  1.2227851193425112,\n",
       "  1.224790799763345,\n",
       "  1.2228183396996104,\n",
       "  1.2112614463753506,\n",
       "  1.220261557781919,\n",
       "  1.2063265529996925,\n",
       "  1.212179814032117,\n",
       "  1.201787057146383,\n",
       "  1.2006685202834493,\n",
       "  1.2034197644048052,\n",
       "  1.199459619964101,\n",
       "  1.196668060417222,\n",
       "  1.1932793379638373,\n",
       "  1.1925735969440479,\n",
       "  1.1919039365693695,\n",
       "  1.189299044870498,\n",
       "  1.190342360735963,\n",
       "  1.182931788198467,\n",
       "  1.1811364383190452,\n",
       "  1.1821691136171615,\n",
       "  1.1783309209531365,\n",
       "  1.179194750622775,\n",
       "  1.1726201551188566,\n",
       "  1.1703078755764802,\n",
       "  1.1724816574008083,\n",
       "  1.1720953478228628,\n",
       "  1.1709751884384236,\n",
       "  1.1642427720377804,\n",
       "  1.1607055339839853,\n",
       "  1.1635377909452067,\n",
       "  1.1614262542743137,\n",
       "  1.1531721080118407,\n",
       "  1.1572023921447703,\n",
       "  1.1595730047976627,\n",
       "  1.1541598698224158,\n",
       "  1.1524962939027932,\n",
       "  1.1451692596608178,\n",
       "  1.1419227689551656,\n",
       "  1.1456188847222335,\n",
       "  1.1461413656992951,\n",
       "  1.142115733110439,\n",
       "  1.139539819367149,\n",
       "  1.1302993470805627,\n",
       "  1.1410599799925358,\n",
       "  1.1260829310937295,\n",
       "  1.1285412037677505,\n",
       "  1.1309704692023972,\n",
       "  1.126288996689869,\n",
       "  1.1258009410552459,\n",
       "  1.1173128982233886,\n",
       "  1.1199206034768454,\n",
       "  1.1219255624016862,\n",
       "  1.114414946814617,\n",
       "  1.1158053529550516,\n",
       "  1.109897091661154,\n",
       "  1.108882329041109,\n",
       "  1.1077347680469862,\n",
       "  1.1084974836708046,\n",
       "  1.105146693091735,\n",
       "  1.1024308380303511,\n",
       "  1.1046549001476713,\n",
       "  1.0972941735908264,\n",
       "  1.0993889822234142,\n",
       "  1.097916249820457,\n",
       "  1.09944607576701,\n",
       "  1.0963728009282259,\n",
       "  1.0866532303525593,\n",
       "  1.0918200303700547,\n",
       "  1.0898608143505792,\n",
       "  1.0830224377192417,\n",
       "  1.0798628675514812,\n",
       "  1.0798002661904322,\n",
       "  1.0801568588099988,\n",
       "  1.0720563653053532,\n",
       "  1.0756797151757445,\n",
       "  1.0801567250667905,\n",
       "  1.0741322384162566,\n",
       "  1.075956670023261,\n",
       "  1.0663436499569727,\n",
       "  1.0671540968007085,\n",
       "  1.0665572205474074,\n",
       "  1.0671514332130292,\n",
       "  1.0611756660433231,\n",
       "  1.0609442314293098,\n",
       "  1.0553441635353877,\n",
       "  1.058452488573884,\n",
       "  1.0559807744183751,\n",
       "  1.0612598587430397,\n",
       "  1.058983639675858,\n",
       "  1.0548591127517233,\n",
       "  1.0522286595727521,\n",
       "  1.0470008958433137,\n",
       "  1.0424120316917607,\n",
       "  1.048136104089545,\n",
       "  1.0429995241452759,\n",
       "  1.0409468243957625,\n",
       "  1.0439202705313013,\n",
       "  1.0369043847450765,\n",
       "  1.0369309238716045,\n",
       "  1.037701368838402,\n",
       "  1.034321800385782,\n",
       "  1.0382531596366706,\n",
       "  1.0359238721814488,\n",
       "  1.0290074804485259,\n",
       "  1.0356495388575935,\n",
       "  1.0282713712550011,\n",
       "  1.0290859493012552,\n",
       "  1.0332780069007816,\n",
       "  1.030440639569532,\n",
       "  1.0204440136211435,\n",
       "  1.0194927485315537,\n",
       "  1.020157252830753,\n",
       "  1.014744977131351,\n",
       "  1.0214021273152034,\n",
       "  1.0164421576474216,\n",
       "  1.0176111950934361,\n",
       "  1.0178085063961362,\n",
       "  1.013308987836246,\n",
       "  1.0139525789321089,\n",
       "  1.010150749018541,\n",
       "  1.0106992892549345,\n",
       "  1.0091193680927566,\n",
       "  1.0040997879000693,\n",
       "  1.00656751168578,\n",
       "  0.9971650221339359,\n",
       "  0.9971505734790916,\n",
       "  1.0080151773498915,\n",
       "  1.000481701191656,\n",
       "  1.001548772424618,\n",
       "  0.9959919834049747,\n",
       "  1.002446071321913,\n",
       "  0.9964626440812059,\n",
       "  0.990194461914539,\n",
       "  0.9987874477930517,\n",
       "  0.9923523650378753,\n",
       "  0.9901891326121529,\n",
       "  0.9878824375541525,\n",
       "  0.9857334103567494,\n",
       "  0.9821610210161307,\n",
       "  0.9814695069626267,\n",
       "  0.9858281882182127,\n",
       "  0.9805553980094567,\n",
       "  0.980021721245292,\n",
       "  0.9861716430886566,\n",
       "  0.9850831122442241,\n",
       "  0.9870205212290767,\n",
       "  0.9838351970948103,\n",
       "  0.9809683300317653,\n",
       "  0.9840654960033025,\n",
       "  0.9752048569641393,\n",
       "  0.9696398479877631,\n",
       "  0.9733478200414539,\n",
       "  0.9767008016892373,\n",
       "  0.9683952721126537,\n",
       "  0.9718014924849554,\n",
       "  0.9694120422468979,\n",
       "  0.9693240804848591,\n",
       "  0.9646683328539495,\n",
       "  0.9658066890362488,\n",
       "  0.9668587866123588,\n",
       "  0.9683443372829724,\n",
       "  0.9659075596138004,\n",
       "  0.9616597399022068,\n",
       "  0.9667590138284079,\n",
       "  0.9607731109285237,\n",
       "  0.9558766326368082,\n",
       "  0.9561821673737637,\n",
       "  0.9586072762562916,\n",
       "  0.9539656957164014,\n",
       "  0.9551256290774065,\n",
       "  0.9583017118330065,\n",
       "  0.9566733201842871,\n",
       "  0.9525754489693132,\n",
       "  0.9459072384738428,\n",
       "  0.9574448060591249,\n",
       "  0.9599938475561177,\n",
       "  0.9529779597022777,\n",
       "  0.9565428401840479,\n",
       "  0.9495353826095578,\n",
       "  0.9532613668660802,\n",
       "  0.9473645808670558,\n",
       "  0.9364511978528898,\n",
       "  0.9488495911327787,\n",
       "  0.9432923885178003,\n",
       "  0.9485607201759251,\n",
       "  0.940283164295536,\n",
       "  0.9462363152903747,\n",
       "  0.9438646645747013,\n",
       "  0.9380507423001214,\n",
       "  0.9432234238824383,\n",
       "  0.9395400063081065,\n",
       "  0.9318052033215721,\n",
       "  0.9330306910364825,\n",
       "  0.9343606590000488,\n",
       "  0.9426377197374032,\n",
       "  0.9363598619311109,\n",
       "  0.9417111166274919,\n",
       "  0.92892543326928,\n",
       "  0.9339064381852822,\n",
       "  0.9353362744478172,\n",
       "  0.9424923621188237,\n",
       "  0.9293911810079329,\n",
       "  0.927249121736662,\n",
       "  0.9251839212129764,\n",
       "  0.9335842200950137,\n",
       "  0.9259881980060162,\n",
       "  0.924793729012698,\n",
       "  0.9318416952737237,\n",
       "  0.924247235883325,\n",
       "  0.9262178023823691,\n",
       "  0.920848190845582,\n",
       "  0.9300533038822696,\n",
       "  0.9246560945590123,\n",
       "  0.9236930763724971,\n",
       "  0.9265023379427504,\n",
       "  0.9182597028065643,\n",
       "  0.9201940478537696,\n",
       "  0.9195645269564882,\n",
       "  0.9185098718291235,\n",
       "  0.9209523755076429,\n",
       "  0.9166926004681685,\n",
       "  0.9127699356708425,\n",
       "  0.9168320742169941,\n",
       "  0.9239322746070953,\n",
       "  0.9158191380071129,\n",
       "  0.9186208203559407,\n",
       "  0.9142967097612993,\n",
       "  0.914430235642332,\n",
       "  0.9184696416247342,\n",
       "  0.9199948512327455,\n",
       "  0.9164174359681986,\n",
       "  0.919663573821729,\n",
       "  0.9116605870679986,\n",
       "  0.9127229049902944,\n",
       "  0.918021567848298,\n",
       "  0.9103282925224103,\n",
       "  0.9112695014295162,\n",
       "  0.9174191946058535,\n",
       "  0.9176386760986756,\n",
       "  0.9110419297567997,\n",
       "  0.9103524309012118,\n",
       "  0.9133686883924632,\n",
       "  0.9119781924817573,\n",
       "  0.9116506476360083,\n",
       "  0.90925641748453,\n",
       "  0.9113550871200659,\n",
       "  0.9068768756856973,\n",
       "  0.9067716877661491,\n",
       "  0.9026896416755782,\n",
       "  0.9033993094210128,\n",
       "  0.900196622827744,\n",
       "  0.904943797217474,\n",
       "  0.9070377999550854,\n",
       "  0.9050113197663393,\n",
       "  0.903998706161662,\n",
       "  0.9001148828174907,\n",
       "  0.9032482261182991,\n",
       "  0.9161482439079173,\n",
       "  0.9040116883665277,\n",
       "  0.8967423929855822,\n",
       "  0.9023433114974406,\n",
       "  0.9083062592110777,\n",
       "  0.9070672640206005,\n",
       "  0.8975917576081651,\n",
       "  0.9024948406483425,\n",
       "  0.8954709137521777,\n",
       "  0.8997247010568649,\n",
       "  0.90207340091258,\n",
       "  0.8977342620934817,\n",
       "  0.9028154106724194,\n",
       "  0.8956212536853861,\n",
       "  0.893800109250874,\n",
       "  0.8939023636221167,\n",
       "  0.8998319703706927,\n",
       "  0.9035819971625304,\n",
       "  0.8976818329657769,\n",
       "  0.9117036121496541,\n",
       "  0.895874244538582,\n",
       "  0.8926478893137993,\n",
       "  0.8991466861686475,\n",
       "  0.9019913520679903,\n",
       "  0.89972014020551,\n",
       "  0.8968124232381686,\n",
       "  0.8925699218632968,\n",
       "  0.8954826769122537,\n",
       "  0.8903125602756827,\n",
       "  0.8989501854005117,\n",
       "  0.8921401563506952,\n",
       "  0.9029741897276791,\n",
       "  0.8925521505440664,\n",
       "  0.8944798591016285,\n",
       "  0.889114104480467,\n",
       "  0.8893712893146275,\n",
       "  0.8954688802935329,\n",
       "  0.8935753952868642,\n",
       "  0.8918541976528717,\n",
       "  0.8928789339391816,\n",
       "  0.8882419890017188,\n",
       "  0.8942853934053913,\n",
       "  0.8949414265996366,\n",
       "  0.8900870034816722,\n",
       "  0.8856597121813156,\n",
       "  0.8866187994956528,\n",
       "  0.8902666178536348,\n",
       "  0.8875903103005839,\n",
       "  0.8947045288154157,\n",
       "  0.893066725900186,\n",
       "  0.8894997235318155,\n",
       "  0.8866273286763606,\n",
       "  0.8827244016734104,\n",
       "  0.8863319333937201,\n",
       "  0.8877009826740625,\n",
       "  0.8839356957120518,\n",
       "  0.8960605707877248,\n",
       "  0.8915001318461456,\n",
       "  0.8844932308451006,\n",
       "  0.8894825842481568,\n",
       "  0.8890028738372381,\n",
       "  0.887958633424113],\n",
       " [5721,\n",
       "  6815,\n",
       "  7089,\n",
       "  7485,\n",
       "  7584,\n",
       "  7799,\n",
       "  7798,\n",
       "  7923,\n",
       "  7903,\n",
       "  7966,\n",
       "  8006,\n",
       "  8127,\n",
       "  8126,\n",
       "  8161,\n",
       "  8149,\n",
       "  8215,\n",
       "  8186,\n",
       "  8233,\n",
       "  8257,\n",
       "  8226,\n",
       "  8222,\n",
       "  8249,\n",
       "  8281,\n",
       "  8235,\n",
       "  8286,\n",
       "  8280,\n",
       "  8276,\n",
       "  8289,\n",
       "  8287,\n",
       "  8307,\n",
       "  8264,\n",
       "  8319,\n",
       "  8248,\n",
       "  8322,\n",
       "  8304,\n",
       "  8326,\n",
       "  8297,\n",
       "  8295,\n",
       "  8318,\n",
       "  8328,\n",
       "  8319,\n",
       "  8292,\n",
       "  8340,\n",
       "  8349,\n",
       "  8352,\n",
       "  8331,\n",
       "  8337,\n",
       "  8343,\n",
       "  8335,\n",
       "  8346,\n",
       "  8352,\n",
       "  8350,\n",
       "  8377,\n",
       "  8369,\n",
       "  8388,\n",
       "  8383,\n",
       "  8357,\n",
       "  8357,\n",
       "  8370,\n",
       "  8376,\n",
       "  8383,\n",
       "  8381,\n",
       "  8374,\n",
       "  8382,\n",
       "  8385,\n",
       "  8380,\n",
       "  8363,\n",
       "  8380,\n",
       "  8386,\n",
       "  8390,\n",
       "  8400,\n",
       "  8382,\n",
       "  8373,\n",
       "  8403,\n",
       "  8396,\n",
       "  8402,\n",
       "  8406,\n",
       "  8401,\n",
       "  8408,\n",
       "  8402,\n",
       "  8391,\n",
       "  8391,\n",
       "  8389,\n",
       "  8393,\n",
       "  8407,\n",
       "  8404,\n",
       "  8419,\n",
       "  8403,\n",
       "  8425,\n",
       "  8420,\n",
       "  8420,\n",
       "  8424,\n",
       "  8426,\n",
       "  8433,\n",
       "  8429,\n",
       "  8435,\n",
       "  8447,\n",
       "  8428,\n",
       "  8441,\n",
       "  8444,\n",
       "  8438,\n",
       "  8451,\n",
       "  8458,\n",
       "  8464,\n",
       "  8453,\n",
       "  8460,\n",
       "  8466,\n",
       "  8470,\n",
       "  8476,\n",
       "  8473,\n",
       "  8466,\n",
       "  8469,\n",
       "  8479,\n",
       "  8481,\n",
       "  8476,\n",
       "  8486,\n",
       "  8478,\n",
       "  8496,\n",
       "  8500,\n",
       "  8488,\n",
       "  8481,\n",
       "  8492,\n",
       "  8490,\n",
       "  8496,\n",
       "  8478,\n",
       "  8506,\n",
       "  8499,\n",
       "  8494,\n",
       "  8503,\n",
       "  8492,\n",
       "  8507,\n",
       "  8509,\n",
       "  8503,\n",
       "  8510,\n",
       "  8508,\n",
       "  8514,\n",
       "  8520,\n",
       "  8523,\n",
       "  8510,\n",
       "  8520,\n",
       "  8530,\n",
       "  8513,\n",
       "  8547,\n",
       "  8526,\n",
       "  8530,\n",
       "  8514,\n",
       "  8528,\n",
       "  8550,\n",
       "  8530,\n",
       "  8532,\n",
       "  8547,\n",
       "  8554,\n",
       "  8553,\n",
       "  8548,\n",
       "  8557,\n",
       "  8556,\n",
       "  8545,\n",
       "  8549,\n",
       "  8543,\n",
       "  8563,\n",
       "  8554,\n",
       "  8562,\n",
       "  8556,\n",
       "  8567,\n",
       "  8553,\n",
       "  8571,\n",
       "  8568,\n",
       "  8567,\n",
       "  8566,\n",
       "  8557,\n",
       "  8566,\n",
       "  8573,\n",
       "  8578,\n",
       "  8584,\n",
       "  8574,\n",
       "  8582,\n",
       "  8594,\n",
       "  8573,\n",
       "  8591,\n",
       "  8588,\n",
       "  8586,\n",
       "  8584,\n",
       "  8579,\n",
       "  8573,\n",
       "  8594,\n",
       "  8585,\n",
       "  8586,\n",
       "  8587,\n",
       "  8576,\n",
       "  8590,\n",
       "  8601,\n",
       "  8603,\n",
       "  8602,\n",
       "  8618,\n",
       "  8589,\n",
       "  8604,\n",
       "  8594,\n",
       "  8605,\n",
       "  8614,\n",
       "  8599,\n",
       "  8625,\n",
       "  8620,\n",
       "  8612,\n",
       "  8631,\n",
       "  8618,\n",
       "  8631,\n",
       "  8638,\n",
       "  8600,\n",
       "  8620,\n",
       "  8612,\n",
       "  8629,\n",
       "  8611,\n",
       "  8621,\n",
       "  8650,\n",
       "  8612,\n",
       "  8624,\n",
       "  8638,\n",
       "  8630,\n",
       "  8634,\n",
       "  8652,\n",
       "  8637,\n",
       "  8631,\n",
       "  8636,\n",
       "  8648,\n",
       "  8619,\n",
       "  8635,\n",
       "  8629,\n",
       "  8638,\n",
       "  8632,\n",
       "  8619,\n",
       "  8638,\n",
       "  8649,\n",
       "  8636,\n",
       "  8643,\n",
       "  8656,\n",
       "  8655,\n",
       "  8638,\n",
       "  8640,\n",
       "  8658,\n",
       "  8657,\n",
       "  8666,\n",
       "  8659,\n",
       "  8664,\n",
       "  8662,\n",
       "  8648,\n",
       "  8655,\n",
       "  8661,\n",
       "  8673,\n",
       "  8669,\n",
       "  8677,\n",
       "  8674,\n",
       "  8665,\n",
       "  8668,\n",
       "  8679,\n",
       "  8687,\n",
       "  8664,\n",
       "  8671,\n",
       "  8672,\n",
       "  8669,\n",
       "  8673,\n",
       "  8666,\n",
       "  8681,\n",
       "  8697,\n",
       "  8684,\n",
       "  8683,\n",
       "  8683,\n",
       "  8689,\n",
       "  8683,\n",
       "  8681,\n",
       "  8681,\n",
       "  8697,\n",
       "  8685,\n",
       "  8694,\n",
       "  8699,\n",
       "  8696,\n",
       "  8686,\n",
       "  8691,\n",
       "  8683,\n",
       "  8690,\n",
       "  8699,\n",
       "  8680,\n",
       "  8681,\n",
       "  8695,\n",
       "  8695,\n",
       "  8708,\n",
       "  8687,\n",
       "  8705,\n",
       "  8693,\n",
       "  8687,\n",
       "  8711,\n",
       "  8704,\n",
       "  8702,\n",
       "  8695,\n",
       "  8691,\n",
       "  8699,\n",
       "  8689,\n",
       "  8712,\n",
       "  8704,\n",
       "  8703,\n",
       "  8701,\n",
       "  8693,\n",
       "  8706,\n",
       "  8713,\n",
       "  8701,\n",
       "  8701,\n",
       "  8707,\n",
       "  8703,\n",
       "  8716,\n",
       "  8710,\n",
       "  8708,\n",
       "  8697,\n",
       "  8696,\n",
       "  8691,\n",
       "  8705,\n",
       "  8708,\n",
       "  8703,\n",
       "  8707,\n",
       "  8705,\n",
       "  8709,\n",
       "  8697,\n",
       "  8714,\n",
       "  8709,\n",
       "  8710,\n",
       "  8707,\n",
       "  8716,\n",
       "  8709,\n",
       "  8719,\n",
       "  8705,\n",
       "  8710,\n",
       "  8713,\n",
       "  8711,\n",
       "  8707,\n",
       "  8707,\n",
       "  8701,\n",
       "  8710,\n",
       "  8705,\n",
       "  8724,\n",
       "  8699,\n",
       "  8695,\n",
       "  8703,\n",
       "  8717,\n",
       "  8715,\n",
       "  8701,\n",
       "  8710,\n",
       "  8710,\n",
       "  8716,\n",
       "  8707,\n",
       "  8705,\n",
       "  8714,\n",
       "  8720,\n",
       "  8711,\n",
       "  8723,\n",
       "  8721,\n",
       "  8716,\n",
       "  8701,\n",
       "  8710,\n",
       "  8710,\n",
       "  8702,\n",
       "  8716,\n",
       "  8714,\n",
       "  8720,\n",
       "  8706,\n",
       "  8712,\n",
       "  8710,\n",
       "  8714,\n",
       "  8722,\n",
       "  8710,\n",
       "  8712,\n",
       "  8715,\n",
       "  8712,\n",
       "  8709,\n",
       "  8719,\n",
       "  8726,\n",
       "  8724,\n",
       "  8715,\n",
       "  8724,\n",
       "  8712,\n",
       "  8717,\n",
       "  8721,\n",
       "  8714,\n",
       "  8715,\n",
       "  8723,\n",
       "  8717,\n",
       "  8723,\n",
       "  8711,\n",
       "  8720,\n",
       "  8713,\n",
       "  8707,\n",
       "  8718,\n",
       "  8722,\n",
       "  8715,\n",
       "  8725,\n",
       "  8717,\n",
       "  8718,\n",
       "  8714,\n",
       "  8717,\n",
       "  8722,\n",
       "  8713,\n",
       "  8724,\n",
       "  8720],\n",
       " [3.027797759220597,\n",
       "  2.5014329328978566,\n",
       "  2.237996395792436,\n",
       "  2.0222212337374628,\n",
       "  1.8978385513278075,\n",
       "  1.771766096067684,\n",
       "  1.721922697237802,\n",
       "  1.6309447715269618,\n",
       "  1.5733629579407835,\n",
       "  1.5255304403957928,\n",
       "  1.4869119829401432,\n",
       "  1.4204535870369162,\n",
       "  1.3896854552472977,\n",
       "  1.359050034466788,\n",
       "  1.3305863331769041,\n",
       "  1.2958965166321565,\n",
       "  1.2683292442947964,\n",
       "  1.2428937885978764,\n",
       "  1.2205411511477033,\n",
       "  1.2039284386410916,\n",
       "  1.1840661146640865,\n",
       "  1.1645315681352297,\n",
       "  1.1480279030297724,\n",
       "  1.1359208880889535,\n",
       "  1.1175682630748378,\n",
       "  1.1072016706082781,\n",
       "  1.0913029410766302,\n",
       "  1.0784475233935291,\n",
       "  1.0634315970897084,\n",
       "  1.0532655149526444,\n",
       "  1.0414846609253996,\n",
       "  1.0249688598492193,\n",
       "  1.0160933396796237,\n",
       "  1.001752289318888,\n",
       "  0.990180040141677,\n",
       "  0.980730624112162,\n",
       "  0.970387889994976,\n",
       "  0.9595259853760788,\n",
       "  0.9498117276657578,\n",
       "  0.9407516207171024,\n",
       "  0.9304270690182892,\n",
       "  0.9209327098550049,\n",
       "  0.9119906039188863,\n",
       "  0.9032500458835182,\n",
       "  0.8944451690524916,\n",
       "  0.8853013973179632,\n",
       "  0.8771577371507697,\n",
       "  0.8686243209140627,\n",
       "  0.8605978053605129,\n",
       "  0.8523684567342278,\n",
       "  0.8459455152501767,\n",
       "  0.8366107858815367,\n",
       "  0.8285613266569597,\n",
       "  0.8210032487073946,\n",
       "  0.8134505103735491,\n",
       "  0.8059227793548385,\n",
       "  0.7985323500722168,\n",
       "  0.7914505888572136,\n",
       "  0.7838987212304267,\n",
       "  0.7772618648007128,\n",
       "  0.7705645920858628,\n",
       "  0.7630611480801118,\n",
       "  0.7561809513866361,\n",
       "  0.7491985402089179,\n",
       "  0.7429463311209806,\n",
       "  0.7365989757988103,\n",
       "  0.7297752508055826,\n",
       "  0.7238545612012732,\n",
       "  0.716736785468308,\n",
       "  0.7107728736477699,\n",
       "  0.7041988496488727,\n",
       "  0.6983670549705612,\n",
       "  0.6921108673054708,\n",
       "  0.6858922789211204,\n",
       "  0.6803230642493978,\n",
       "  0.674360831156485,\n",
       "  0.6685325264049033,\n",
       "  0.6627581249675453,\n",
       "  0.6567828518508327,\n",
       "  0.6511322888943507,\n",
       "  0.6459631831706811,\n",
       "  0.640204392398049,\n",
       "  0.6349410400741502,\n",
       "  0.6296673356010697,\n",
       "  0.6238482121723165,\n",
       "  0.6187196070925978,\n",
       "  0.61331836954734,\n",
       "  0.6081809942235314,\n",
       "  0.6031706199860624,\n",
       "  0.5979983286689409,\n",
       "  0.5929995863014221,\n",
       "  0.5880639197072461,\n",
       "  0.5829194831501553,\n",
       "  0.578156805311088,\n",
       "  0.5731255864657347,\n",
       "  0.5683515220514297,\n",
       "  0.5637981645758131,\n",
       "  0.5590218851298191,\n",
       "  0.5543584045179566,\n",
       "  0.5496910874170232,\n",
       "  0.5453438688856349,\n",
       "  0.5405732496263735,\n",
       "  0.5364701243500235,\n",
       "  0.5317091094716582,\n",
       "  0.5273831821504722,\n",
       "  0.5231562034257479,\n",
       "  0.5186811517338918,\n",
       "  0.5144199176089765,\n",
       "  0.5102729416325604,\n",
       "  0.5061637075143591,\n",
       "  0.5020616754017996,\n",
       "  0.4979072447860339,\n",
       "  0.4939357456858231,\n",
       "  0.4898904732193841,\n",
       "  0.4860046329168002,\n",
       "  0.48194147858775316,\n",
       "  0.47818213478908284,\n",
       "  0.4742017252982577,\n",
       "  0.4704483764364131,\n",
       "  0.4666244568496914,\n",
       "  0.46303402164082397,\n",
       "  0.4592948757638029,\n",
       "  0.45562638121858207,\n",
       "  0.45208241565385787,\n",
       "  0.4486547515001901,\n",
       "  0.4449536848318458,\n",
       "  0.44140917680798497,\n",
       "  0.4381894360435512,\n",
       "  0.4344592020373395,\n",
       "  0.4311610558477954,\n",
       "  0.42775233753428005,\n",
       "  0.4246008545301109,\n",
       "  0.421239939522865,\n",
       "  0.417877483897602,\n",
       "  0.4146441492203751,\n",
       "  0.41141634299058116,\n",
       "  0.4088335148851662,\n",
       "  0.4051565622581196,\n",
       "  0.40216280431617824,\n",
       "  0.39907443024305567,\n",
       "  0.3960430994076747,\n",
       "  0.39309667134253945,\n",
       "  0.39007693700216095,\n",
       "  0.3870472009886311,\n",
       "  0.384393210862086,\n",
       "  0.381303095310367,\n",
       "  0.3783467007605099,\n",
       "  0.3755687845739497,\n",
       "  0.37285964947545114,\n",
       "  0.3699875949342655,\n",
       "  0.3671508208484225,\n",
       "  0.36445712483964604,\n",
       "  0.3617583683310402,\n",
       "  0.3590683432484201,\n",
       "  0.35642827166425534,\n",
       "  0.3539143182940666,\n",
       "  0.3512305527789328,\n",
       "  0.3487406896582103,\n",
       "  0.3462141415269685,\n",
       "  0.34365616928772863,\n",
       "  0.3412420776279528,\n",
       "  0.3389241268065339,\n",
       "  0.3363279602537986,\n",
       "  0.334083385598353,\n",
       "  0.3316247015299987,\n",
       "  0.32926114895111575,\n",
       "  0.3268669090929788,\n",
       "  0.3247043366732185,\n",
       "  0.3224736622419113,\n",
       "  0.3199986308664038,\n",
       "  0.31779941164549896,\n",
       "  0.31562884927422935,\n",
       "  0.31336295092483296,\n",
       "  0.31120625160293075,\n",
       "  0.308969972530441,\n",
       "  0.30702860643707347,\n",
       "  0.30479040741989577,\n",
       "  0.30271495384629715,\n",
       "  0.3007640114576164,\n",
       "  0.29883403945127096,\n",
       "  0.29662136062280514,\n",
       "  0.2945544453507788,\n",
       "  0.29262602475512123,\n",
       "  0.29071601767827265,\n",
       "  0.2887902960903824,\n",
       "  0.28691937529172656,\n",
       "  0.284917324203225,\n",
       "  0.2829383659273609,\n",
       "  0.281406014374416,\n",
       "  0.2795289473596926,\n",
       "  0.27752618639485704,\n",
       "  0.27561686264899216,\n",
       "  0.27385322384461913,\n",
       "  0.2719674761719056,\n",
       "  0.2703981789533129,\n",
       "  0.2684857384661869,\n",
       "  0.2668695150632619,\n",
       "  0.2651619648120479,\n",
       "  0.26338832747058916,\n",
       "  0.26180050383688436,\n",
       "  0.2600780594700397,\n",
       "  0.2585298025465882,\n",
       "  0.2569095948747536,\n",
       "  0.255312431825127,\n",
       "  0.2535635375386447,\n",
       "  0.25208952629154446,\n",
       "  0.25051828328271836,\n",
       "  0.24897208383902372,\n",
       "  0.24737411206316828,\n",
       "  0.2459755705334524,\n",
       "  0.24436531021734179,\n",
       "  0.24304504894171225,\n",
       "  0.2414374929805413,\n",
       "  0.2400238409768686,\n",
       "  0.23850923090191606,\n",
       "  0.23707621101153334,\n",
       "  0.23593508920220269,\n",
       "  0.2342742506531517,\n",
       "  0.23281109527219435,\n",
       "  0.2315134726674276,\n",
       "  0.23037649513790814,\n",
       "  0.22896662098528026,\n",
       "  0.22755147490583855,\n",
       "  0.22628080453310218,\n",
       "  0.22494675876812115,\n",
       "  0.2235803637174242,\n",
       "  0.22224183935553404,\n",
       "  0.22110015132498792,\n",
       "  0.2196530389133049,\n",
       "  0.21867502678312956,\n",
       "  0.21719260579082364,\n",
       "  0.21611054136934713,\n",
       "  0.21476719561963836,\n",
       "  0.21370602590304932,\n",
       "  0.21246733772937348,\n",
       "  0.21113488396713587,\n",
       "  0.21007083556615863,\n",
       "  0.20883568892705023,\n",
       "  0.207654760715245,\n",
       "  0.20674169496051722,\n",
       "  0.2055334718005171,\n",
       "  0.20460662356621043,\n",
       "  0.20316966421664223,\n",
       "  0.2020188810725021,\n",
       "  0.2012589579249307,\n",
       "  0.20004254609476047,\n",
       "  0.1989303227581976,\n",
       "  0.1977645942748555,\n",
       "  0.19672306382226498,\n",
       "  0.1957352931909786,\n",
       "  0.19468368774994688,\n",
       "  0.1938011485373755,\n",
       "  0.19287867330538158,\n",
       "  0.19167527020965874,\n",
       "  0.1907502595390021,\n",
       "  0.1897954859147698,\n",
       "  0.18900573276424334,\n",
       "  0.18797480880723558,\n",
       "  0.18685767584741858,\n",
       "  0.18611182962517578,\n",
       "  0.18503362276338614,\n",
       "  0.18398166225500004,\n",
       "  0.18338383312028164,\n",
       "  0.1822141444229108,\n",
       "  0.18137853992131345,\n",
       "  0.18048129324161755,\n",
       "  0.17949726404861924,\n",
       "  0.17869119729679497,\n",
       "  0.17794634941338144,\n",
       "  0.17694798255788471,\n",
       "  0.17614439873130197,\n",
       "  0.1751704879372315,\n",
       "  0.1746633758992368,\n",
       "  0.17365738513167958,\n",
       "  0.1728419152148384,\n",
       "  0.17222617181902342,\n",
       "  0.17117573422691768,\n",
       "  0.1704773677478055,\n",
       "  0.16969391985225674,\n",
       "  0.16887118842092222,\n",
       "  0.16808164407200574,\n",
       "  0.1677456597717665,\n",
       "  0.16666798549023243,\n",
       "  0.16590469045924347,\n",
       "  0.16511798213286466,\n",
       "  0.16443210690431756,\n",
       "  0.16393462054266286,\n",
       "  0.16288995390075175,\n",
       "  0.16226000976449656,\n",
       "  0.1614521881717335,\n",
       "  0.16082079371683825,\n",
       "  0.1601681019599794,\n",
       "  0.15947300157667227,\n",
       "  0.1587235256402209,\n",
       "  0.15807196219275205,\n",
       "  0.15766147653491083,\n",
       "  0.15689425650019725,\n",
       "  0.15607274225165635,\n",
       "  0.15531434476123585,\n",
       "  0.15470365321981824,\n",
       "  0.1541041827222804,\n",
       "  0.1533992411314672,\n",
       "  0.15295645854904982,\n",
       "  0.1521360581856786,\n",
       "  0.15161345472526244,\n",
       "  0.1510293928947568,\n",
       "  0.15033933219623505,\n",
       "  0.14991601861975576,\n",
       "  0.1491808878218604,\n",
       "  0.14850363981331643,\n",
       "  0.14799477375353626,\n",
       "  0.14756308953422992,\n",
       "  0.14682356971567567,\n",
       "  0.14627173658618353,\n",
       "  0.14568822261124678,\n",
       "  0.14518091741687242,\n",
       "  0.1447845104254243,\n",
       "  0.14410221474800058,\n",
       "  0.14356766095238327,\n",
       "  0.14307765013792945,\n",
       "  0.142505403111908,\n",
       "  0.14233437895086545,\n",
       "  0.14170917567032765,\n",
       "  0.14113259451603272,\n",
       "  0.14050406342155958,\n",
       "  0.13987849770027433,\n",
       "  0.1395974560051687,\n",
       "  0.1389585935072454,\n",
       "  0.13846419415457617,\n",
       "  0.13797326401498794,\n",
       "  0.13758515524656142,\n",
       "  0.1370792950248639,\n",
       "  0.13662868413411694,\n",
       "  0.13622135465614627,\n",
       "  0.13567086993330268,\n",
       "  0.13534950519125294,\n",
       "  0.13495062442055866,\n",
       "  0.13437461469097997,\n",
       "  0.13415942754981242,\n",
       "  0.13352789493984027,\n",
       "  0.13315757350292676,\n",
       "  0.13273775757874107,\n",
       "  0.1323201430947706,\n",
       "  0.13187991319185244,\n",
       "  0.13177609345130717,\n",
       "  0.13107768589175015,\n",
       "  0.13062145928660604,\n",
       "  0.1301413000442063,\n",
       "  0.13006506920613328,\n",
       "  0.12940280259818535,\n",
       "  0.12906815380745962,\n",
       "  0.12861200031527473,\n",
       "  0.12819754395342178,\n",
       "  0.12784640700966526,\n",
       "  0.1274647031938712,\n",
       "  0.12705072518531924,\n",
       "  0.12663740408080087,\n",
       "  0.12654005348687677,\n",
       "  0.12586004702669776,\n",
       "  0.1256529696425576,\n",
       "  0.1252509821427672,\n",
       "  0.1251254294534273,\n",
       "  0.12465107572614748,\n",
       "  0.124345539160177,\n",
       "  0.12384953206114367,\n",
       "  0.123575530274965,\n",
       "  0.12315649130513856,\n",
       "  0.12287239552085882,\n",
       "  0.12256866867729457,\n",
       "  0.12220809127546625,\n",
       "  0.12177312977399574,\n",
       "  0.12168243841422469,\n",
       "  0.12124116639413533,\n",
       "  0.12090381154697459,\n",
       "  0.12051248628586132,\n",
       "  0.12020707444450561,\n",
       "  0.11989831854973114,\n",
       "  0.11968068006580385,\n",
       "  0.11938216390430997,\n",
       "  0.1190373289927282,\n",
       "  0.11881958789089102,\n",
       "  0.11838857196754554,\n",
       "  0.11816445212818887,\n",
       "  0.11801677487960248,\n",
       "  0.11766266195094383,\n",
       "  0.11746399423228009,\n",
       "  0.11730440066942864,\n",
       "  0.11676858405874609,\n",
       "  0.11664037868449013,\n",
       "  0.11614386321342993,\n",
       "  0.11606074696625847,\n",
       "  0.11563566166037507,\n",
       "  0.11544395999465674,\n",
       "  0.11526712477196588,\n",
       "  0.11519553969956092,\n",
       "  0.11476842319201588,\n",
       "  0.11452397298586602,\n",
       "  0.11450059621308757,\n",
       "  0.11395391516616535,\n",
       "  0.11357040558366854],\n",
       " [649,\n",
       "  786,\n",
       "  849,\n",
       "  893,\n",
       "  916,\n",
       "  933,\n",
       "  940,\n",
       "  947,\n",
       "  952,\n",
       "  960,\n",
       "  963,\n",
       "  970,\n",
       "  977,\n",
       "  979,\n",
       "  982,\n",
       "  988,\n",
       "  989,\n",
       "  992,\n",
       "  992,\n",
       "  991,\n",
       "  992,\n",
       "  992,\n",
       "  994,\n",
       "  993,\n",
       "  994,\n",
       "  994,\n",
       "  994,\n",
       "  994,\n",
       "  994,\n",
       "  994,\n",
       "  995,\n",
       "  996,\n",
       "  997,\n",
       "  997,\n",
       "  998,\n",
       "  998,\n",
       "  998,\n",
       "  998,\n",
       "  998,\n",
       "  999,\n",
       "  998,\n",
       "  999,\n",
       "  999,\n",
       "  1000,\n",
       "  999,\n",
       "  1000,\n",
       "  999,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing how regularization affects the performance of neural netowork using regularization parameter = 0.1\n",
    "import mnist_loader \n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper() \n",
    "\n",
    "import network2 \n",
    "net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
    "net.large_weight_initializer()\n",
    "net.SGD(training_data[:1000], 400, 10, 0.5,evaluation_data=test_data, lmbda = 0.1,\n",
    "        monitor_evaluation_cost=True, monitor_evaluation_accuracy=True,\n",
    "        monitor_training_cost=True, monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on training data: 45668 / 50000\n",
      "Accuracy on evaluation data: 9146 / 10000\n",
      "\n",
      "Epoch 1 training complete\n",
      "Accuracy on training data: 46668 / 50000\n",
      "Accuracy on evaluation data: 9311 / 10000\n",
      "\n",
      "Epoch 2 training complete\n",
      "Accuracy on training data: 47301 / 50000\n",
      "Accuracy on evaluation data: 9400 / 10000\n",
      "\n",
      "Epoch 3 training complete\n",
      "Accuracy on training data: 47381 / 50000\n",
      "Accuracy on evaluation data: 9428 / 10000\n",
      "\n",
      "Epoch 4 training complete\n",
      "Accuracy on training data: 47573 / 50000\n",
      "Accuracy on evaluation data: 9423 / 10000\n",
      "\n",
      "Epoch 5 training complete\n",
      "Accuracy on training data: 47865 / 50000\n",
      "Accuracy on evaluation data: 9501 / 10000\n",
      "\n",
      "Epoch 6 training complete\n",
      "Accuracy on training data: 47965 / 50000\n",
      "Accuracy on evaluation data: 9522 / 10000\n",
      "\n",
      "Epoch 7 training complete\n",
      "Accuracy on training data: 47896 / 50000\n",
      "Accuracy on evaluation data: 9528 / 10000\n",
      "\n",
      "Epoch 8 training complete\n",
      "Accuracy on training data: 48141 / 50000\n",
      "Accuracy on evaluation data: 9553 / 10000\n",
      "\n",
      "Epoch 9 training complete\n",
      "Accuracy on training data: 48163 / 50000\n",
      "Accuracy on evaluation data: 9560 / 10000\n",
      "\n",
      "Epoch 10 training complete\n",
      "Accuracy on training data: 48287 / 50000\n",
      "Accuracy on evaluation data: 9599 / 10000\n",
      "\n",
      "Epoch 11 training complete\n",
      "Accuracy on training data: 48182 / 50000\n",
      "Accuracy on evaluation data: 9575 / 10000\n",
      "\n",
      "Epoch 12 training complete\n",
      "Accuracy on training data: 47992 / 50000\n",
      "Accuracy on evaluation data: 9533 / 10000\n",
      "\n",
      "Epoch 13 training complete\n",
      "Accuracy on training data: 48262 / 50000\n",
      "Accuracy on evaluation data: 9556 / 10000\n",
      "\n",
      "Epoch 14 training complete\n",
      "Accuracy on training data: 48463 / 50000\n",
      "Accuracy on evaluation data: 9622 / 10000\n",
      "\n",
      "Epoch 15 training complete\n",
      "Accuracy on training data: 48204 / 50000\n",
      "Accuracy on evaluation data: 9571 / 10000\n",
      "\n",
      "Epoch 16 training complete\n",
      "Accuracy on training data: 48309 / 50000\n",
      "Accuracy on evaluation data: 9584 / 10000\n",
      "\n",
      "Epoch 17 training complete\n",
      "Accuracy on training data: 48015 / 50000\n",
      "Accuracy on evaluation data: 9536 / 10000\n",
      "\n",
      "Epoch 18 training complete\n",
      "Accuracy on training data: 48330 / 50000\n",
      "Accuracy on evaluation data: 9589 / 10000\n",
      "\n",
      "Epoch 19 training complete\n",
      "Accuracy on training data: 48330 / 50000\n",
      "Accuracy on evaluation data: 9601 / 10000\n",
      "\n",
      "Epoch 20 training complete\n",
      "Accuracy on training data: 48422 / 50000\n",
      "Accuracy on evaluation data: 9601 / 10000\n",
      "\n",
      "Epoch 21 training complete\n",
      "Accuracy on training data: 48136 / 50000\n",
      "Accuracy on evaluation data: 9560 / 10000\n",
      "\n",
      "Epoch 22 training complete\n",
      "Accuracy on training data: 48458 / 50000\n",
      "Accuracy on evaluation data: 9613 / 10000\n",
      "\n",
      "Epoch 23 training complete\n",
      "Accuracy on training data: 48492 / 50000\n",
      "Accuracy on evaluation data: 9610 / 10000\n",
      "\n",
      "Epoch 24 training complete\n",
      "Accuracy on training data: 48134 / 50000\n",
      "Accuracy on evaluation data: 9585 / 10000\n",
      "\n",
      "Epoch 25 training complete\n",
      "Accuracy on training data: 48393 / 50000\n",
      "Accuracy on evaluation data: 9609 / 10000\n",
      "\n",
      "Epoch 26 training complete\n",
      "Accuracy on training data: 48394 / 50000\n",
      "Accuracy on evaluation data: 9628 / 10000\n",
      "\n",
      "Epoch 27 training complete\n",
      "Accuracy on training data: 48446 / 50000\n",
      "Accuracy on evaluation data: 9632 / 10000\n",
      "\n",
      "Epoch 28 training complete\n",
      "Accuracy on training data: 48304 / 50000\n",
      "Accuracy on evaluation data: 9575 / 10000\n",
      "\n",
      "Epoch 29 training complete\n",
      "Accuracy on training data: 48425 / 50000\n",
      "Accuracy on evaluation data: 9582 / 10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [9146,\n",
       "  9311,\n",
       "  9400,\n",
       "  9428,\n",
       "  9423,\n",
       "  9501,\n",
       "  9522,\n",
       "  9528,\n",
       "  9553,\n",
       "  9560,\n",
       "  9599,\n",
       "  9575,\n",
       "  9533,\n",
       "  9556,\n",
       "  9622,\n",
       "  9571,\n",
       "  9584,\n",
       "  9536,\n",
       "  9589,\n",
       "  9601,\n",
       "  9601,\n",
       "  9560,\n",
       "  9613,\n",
       "  9610,\n",
       "  9585,\n",
       "  9609,\n",
       "  9628,\n",
       "  9632,\n",
       "  9575,\n",
       "  9582],\n",
       " [],\n",
       " [45668,\n",
       "  46668,\n",
       "  47301,\n",
       "  47381,\n",
       "  47573,\n",
       "  47865,\n",
       "  47965,\n",
       "  47896,\n",
       "  48141,\n",
       "  48163,\n",
       "  48287,\n",
       "  48182,\n",
       "  47992,\n",
       "  48262,\n",
       "  48463,\n",
       "  48204,\n",
       "  48309,\n",
       "  48015,\n",
       "  48330,\n",
       "  48330,\n",
       "  48422,\n",
       "  48136,\n",
       "  48458,\n",
       "  48492,\n",
       "  48134,\n",
       "  48393,\n",
       "  48394,\n",
       "  48446,\n",
       "  48304,\n",
       "  48425])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.large_weight_initializer()\n",
    "net.SGD(training_data, 30, 10, 0.5, \n",
    "        evaluation_data=test_data, lmbda = 5.0, \n",
    "        monitor_evaluation_accuracy=True, monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on evaluation data: 9440 / 10000\n",
      "\n",
      "Epoch 1 training complete\n",
      "Accuracy on evaluation data: 9572 / 10000\n",
      "\n",
      "Epoch 2 training complete\n",
      "Accuracy on evaluation data: 9605 / 10000\n",
      "\n",
      "Epoch 3 training complete\n",
      "Accuracy on evaluation data: 9652 / 10000\n",
      "\n",
      "Epoch 4 training complete\n",
      "Accuracy on evaluation data: 9683 / 10000\n",
      "\n",
      "Epoch 5 training complete\n",
      "Accuracy on evaluation data: 9725 / 10000\n",
      "\n",
      "Epoch 6 training complete\n",
      "Accuracy on evaluation data: 9722 / 10000\n",
      "\n",
      "Epoch 7 training complete\n",
      "Accuracy on evaluation data: 9730 / 10000\n",
      "\n",
      "Epoch 8 training complete\n",
      "Accuracy on evaluation data: 9702 / 10000\n",
      "\n",
      "Epoch 9 training complete\n",
      "Accuracy on evaluation data: 9765 / 10000\n",
      "\n",
      "Epoch 10 training complete\n",
      "Accuracy on evaluation data: 9734 / 10000\n",
      "\n",
      "Epoch 11 training complete\n",
      "Accuracy on evaluation data: 9678 / 10000\n",
      "\n",
      "Epoch 12 training complete\n",
      "Accuracy on evaluation data: 9752 / 10000\n",
      "\n",
      "Epoch 13 training complete\n",
      "Accuracy on evaluation data: 9758 / 10000\n",
      "\n",
      "Epoch 14 training complete\n",
      "Accuracy on evaluation data: 9761 / 10000\n",
      "\n",
      "Epoch 15 training complete\n",
      "Accuracy on evaluation data: 9669 / 10000\n",
      "\n",
      "Epoch 16 training complete\n",
      "Accuracy on evaluation data: 9761 / 10000\n",
      "\n",
      "Epoch 17 training complete\n",
      "Accuracy on evaluation data: 9744 / 10000\n",
      "\n",
      "Epoch 18 training complete\n",
      "Accuracy on evaluation data: 9747 / 10000\n",
      "\n",
      "Epoch 19 training complete\n",
      "Accuracy on evaluation data: 9723 / 10000\n",
      "\n",
      "Epoch 20 training complete\n",
      "Accuracy on evaluation data: 9729 / 10000\n",
      "\n",
      "Epoch 21 training complete\n",
      "Accuracy on evaluation data: 9703 / 10000\n",
      "\n",
      "Epoch 22 training complete\n",
      "Accuracy on evaluation data: 9680 / 10000\n",
      "\n",
      "Epoch 23 training complete\n",
      "Accuracy on evaluation data: 9743 / 10000\n",
      "\n",
      "Epoch 24 training complete\n",
      "Accuracy on evaluation data: 9755 / 10000\n",
      "\n",
      "Epoch 25 training complete\n",
      "Accuracy on evaluation data: 9764 / 10000\n",
      "\n",
      "Epoch 26 training complete\n",
      "Accuracy on evaluation data: 9750 / 10000\n",
      "\n",
      "Epoch 27 training complete\n",
      "Accuracy on evaluation data: 9754 / 10000\n",
      "\n",
      "Epoch 28 training complete\n",
      "Accuracy on evaluation data: 9765 / 10000\n",
      "\n",
      "Epoch 29 training complete\n",
      "Accuracy on evaluation data: 9756 / 10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [9440,\n",
       "  9572,\n",
       "  9605,\n",
       "  9652,\n",
       "  9683,\n",
       "  9725,\n",
       "  9722,\n",
       "  9730,\n",
       "  9702,\n",
       "  9765,\n",
       "  9734,\n",
       "  9678,\n",
       "  9752,\n",
       "  9758,\n",
       "  9761,\n",
       "  9669,\n",
       "  9761,\n",
       "  9744,\n",
       "  9747,\n",
       "  9723,\n",
       "  9729,\n",
       "  9703,\n",
       "  9680,\n",
       "  9743,\n",
       "  9755,\n",
       "  9764,\n",
       "  9750,\n",
       "  9754,\n",
       "  9765,\n",
       "  9756],\n",
       " [],\n",
       " [])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = network2.Network([784, 100, 10], cost=network2.CrossEntropyCost)\n",
    "net.large_weight_initializer()\n",
    "net.SGD(training_data, 30, 10, 0.5, \n",
    "        lmbda=5.0, \n",
    "        evaluation_data=validation_data, \n",
    "        monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
